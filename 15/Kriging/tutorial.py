# -*- coding: utf-8 -*-
"""SMT_Tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Qy6YHWJYx2osAdr0Bv3b4XgX2Nr57IT

<a href="https://colab.research.google.com/github/SMTorg/smt/blob/master/tutorial/SMT_Tutorial.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

<div class="jumbotron text-left"><b>
This tutorial describes how to use the SMT toolbox, which is a python toolbox for building a surrogate model.</b></div>

Nathalie BARTOLI ONERA/DTIS/M2CI - May 2024

based on `SMT 2.5.1 version`

<div class="alert alert-info fade in" id="d110">
<p>Latest updates</p>
<ol> - sparse gaussian process for large database </ol>
<ol> - use GPX build in Rust in order to reduce time for trining and prediction (KRG or KPLS models) </ol>
    
 <p>Previous updates</p>  
<ol>     -correct the figures with matplotlib:  ax =  fig.add_subplot(projection='3d') </ol>
<ol> - prediction and variance derivatives for different kernels (squared exponential, absolute exponential, matern 32, matern 52) and trends (constant, linear) </ol>
<ol> -  Kriging with integer variables </ol>   
<ol> -  compare different covariance kernels for the kriging models </ol>
<ol> -  automatic choice of PLS comonents </ol>
<ol> -  example to use Marginal Gaussian Process   </ol>    
<ol> -  example to use multiple outputs (independant outputs) </ol>
<ol> -  example to compare models using Mixture of experts technique  </ol>  
<ol> -  example to save and load models using  `pickle`  </ol>    

</div>

<p class="alert alert-success" style="padding:1em">
To use SMT models, please follow this link : https://github.com/SMTorg/SMT/blob/master/README.md. The documentation is available here: http://smt.readthedocs.io/en/latest/
</p>

The reference paper is available
here https://www.sciencedirect.com/science/article/pii/S0965997818309360?via%3Dihub

or as a preprint: https://www.researchgate.net/profile/Mohamed_Amine_Bouhlel/publication/331976718_A_Python_surrogate_modeling_framework_with_derivatives/links/5cc3cebd299bf12097829631/A-Python-surrogate-modeling-framework-with-derivatives.pdf

Cite us:

M.-A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Morlier, J .R.R.A Martins (2019), A Python surrogate modeling framework with derivatives, Advances in Engineering Software, 102662

P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, J. RRA. Martins (2024). SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes. Advances in Engineering Software, 188, 103571.

@article{SMT2019,
  title={A Python surrogate modeling framework with derivatives},
  author={Mohamed Amine Bouhlel and John T. Hwang and Nathalie Bartoli and R{\'e}mi Lafage and Joseph Morlier and Joaquim R. R. A. Martins},
  journal={Advances in Engineering Software},
  pages={102662},
  year={2019},
  publisher={Elsevier}
}

@article{saves2024smt,
  title={SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes},
  author={Saves, Paul and Lafage, R{\'e}mi and Bartoli, Nathalie and Diouane, Youssef and Bussemaker, Jasper and Lefebvre, Thierry and Hwang, John T and Morlier, Joseph and Martins, Joaquim RRA},
  journal={Advances in Engineering Software},
  volume={188},
  pages={103571},
  year={2024},
  publisher={Elsevier}
}

<div class="alert alert-danger" >
<p> In most of the surrogate models $y\in\mathbb{R}$, so if you have multiple outputs $y\in\mathbb{R}^d$ (which are considered as independant outputs), add an external loop to build one surrogate model per output. The correlations betweens outputs are not taken into account. Section 11 of this notebook presents an example with 2 outputs.
</div>

<div class="alert alert-warning" >
In this notebook, the database is considered to be noise-free. If you want to consider noisy data, please have a look at the dedicated notebook available within SMT.
</div>
"""

from __future__ import print_function, division

# !pip install smt
# # to reduce CPU time
# !pip install numba

"""<div class="alert alert-warning" >
If you use hierarchical variables and the size of your doe greater than 30 points, you may leverage the `numba` JIT compiler to speed up the computation
To do so:
    
 - install numba library
    
     `pip install numba`
    
    
 - and define the environment variable `USE_NUMBA_JIT = 1` (unset or 0 if you do not want to use numba)
    
     - Linux: export USE_NUMBA_JIT = 1
    
     - Windows: set USE_NUMBA_JIT = 1

</div>
"""

# # to check if numba is available
# !pip show numba
# # and then you need to define the environment variable USE_NUMBA_JIT
# !echo "Numba used or not in your environment=" %USE_NUMBA_JIT%

"""# 0. Construction of the DOE points"""

import numpy as np
from smt.utils.misc import compute_rms_error

from smt.problems import Rosenbrock
from smt.sampling_methods import LHS
from smt.surrogate_models import LS, QP, KPLS, KRG, KPLSK, GEKPLS, MGP

try:
    from smt.surrogate_models import IDW, RBF, RMTC, RMTB

    compiled_available = True
except Exception:
    compiled_available = False

try:
    import matplotlib.pyplot as plt

    plot_status = True
except Exception:
    plot_status = False


import matplotlib.pyplot as plt
from matplotlib import cm


# to ignore warning messages
import warnings

warnings.filterwarnings("ignore")

"""## Rosenbrock Function  in dimension N

$$
f(\mathbf{x}) = \sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2 \quad \mbox{where} \quad \mathbf{x} = [x_1, \ldots, x_N] \in \mathbb{R}^N.
$$

$$x_i \in [-2,2]$$

## Training points and validation points using a LHS algorithm

Here we outline the studied function by plotting the surface representing the function. In addition, we plot also the training points used later to build the surrogate model as well as the validation points which will be used to evaluate the quality of the surrogate model. Both the training point and validation points are generated by a LHS DOE algorithm.

<div class="alert alert-danger" >
<p> In order to have reproducibility of the tests and results, we use  the option random_state to set a seed to the random generator, so that your DOE points are always deterministic. If you don't set a seed, it is different each time.
</div>
"""

########### Initialization of the problem, construction of the training and validation points

ndim = 2
ndoe = 200  # int(10*ndim)
# Define the function
fun = Rosenbrock(ndim=ndim)

# Construction of the DOE
# in order to have the always same LHS points, random_state=1
sampling = LHS(xlimits=fun.xlimits, criterion="ese", random_state=1)
xt = sampling(ndoe)
# Compute the outputs
yt = fun(xt)

# Construction of the validation points
ntest = 200  # 500
sampling = LHS(xlimits=fun.xlimits, criterion="ese", random_state=1)
xtest = sampling(ntest)
ytest = fun(xtest)

# To visualize the DOE points
fig = plt.figure(figsize=(10, 10))
plt.scatter(xt[:, 0], xt[:, 1], marker="x", c="b", s=200, label="Training points")
plt.scatter(
    xtest[:, 0], xtest[:, 1], marker=".", c="k", s=200, label="Validation points"
)
plt.title("DOE")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('DOE.png')

# To plot the Rosenbrock function
x = np.linspace(-2, 2, 50)
res = []
for x0 in x:
    for x1 in x:
        res.append(fun(np.array([[x0, x1]])))
res = np.array(res)
res = res.reshape((50, 50)).T
X, Y = np.meshgrid(x, x)
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
surf = ax.plot_surface(
    X, Y, res, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
)

ax.scatter(
    xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="Training point"
)
ax.scatter(
    xtest[:, 0],
    xtest[:, 1],
    ytest,
    zdir="z",
    marker=".",
    c="k",
    s=200,
    label="Validation point",
)

plt.title("Rosenbrock function")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('Rosenbrock function.png')

"""Different models will be used and compared:

- Linear Model (first order polynomial)
- Quadratic Model (second order polynomial)
- Kriging Model (also known as Gaussian Process)
- KPLS Model and KPLSK Model (useful for Kriging in high dimension)
- IDW Model (Inverse Distance Weighting)
- RBF Model (Radial Basis Function)
- RMTS Models: RMTB and RMTC (useful for low dimensional problem)

Some metrics to assess some errors are proposed at the end.

Mixture of experts technique will be used to compare different surrogate models.


"""# 3. Kriging Model"""

########### The Kriging model

# The variable 'theta0' is a list of length ndim.
t = KRG(theta0=[1e-2] * ndim, print_prediction=False)
t.set_training_values(xt, yt[:, 0])

t.train()

# Prediction of the validation points
y = t.predict_values(xtest)
print("Kriging,  err: " + str(compute_rms_error(t, xtest, ytest)))
if plot_status:
    # Plot the function and the prediction
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("Kriging model: validation of the prediction model")
    

if plot_status:
    plt.savefig('Kriging model: validation of the prediction model.png')

# Value of theta
print("theta values", t.optimal_theta)

"""Associated to the Kriging prediction, there is the estimated variance in order to have a confidence interval.

"""

# estimated variance for the validation points
s2 = t.predict_variances(xtest)
# plot with the associated interval confidence
yerr = (
    2 * 3 * np.sqrt(s2)
)  # in order to use +/- 3 x standard deviation: 99% confidence interval estimation
if plot_status:
    # Plot the function, the prediction and the 99% confidence interval based on
    # the MSE
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")
    plt.errorbar(
        np.squeeze(ytest),
        np.squeeze(y),
        yerr=np.squeeze(yerr),
        fmt="none",
        capsize=5,
        ecolor="lightgray",
        elinewidth=1,
        capthick=0.5,
        label="confidence estimate 99%",
    )
    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title(
        "Kriging model: validation of the prediction model with the estimate of confidence"
    )

if plot_status:
    plt.savefig('Kriging model: validation of the prediction model with the estimate of confidence.png')

"""You can also have access to the kriging derivative prediction

"""

# estimated derivative for the validation points
dydx1 = t.predict_derivatives(xtest, 0)  # derivative according to the x1
dydx2 = t.predict_derivatives(xtest, 1)  # derivative according to the x2

# estimated variance derivative for the validation points
dsigmadx1 = t.predict_variance_derivatives(    xtest, 0)  # variance derivative according to the x1
dsigmadx2 = t.predict_variance_derivatives(
    xtest, 1
)  # variance derivative according to the x2

"""<div class="alert alert-info fade in" id="d110">
<p>The idea could be to compare different covariance kernels</p>
<ol> -  a squared exponential kernel (by default)  </ol>
<ol> -  an absolute exponential kernel   </ol>    
<ol> - some Matern Kernels (matern 32 and matern 52) </ol>
</div>
"""

# squared exponential by default
t1 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp")
t1.set_training_values(xt, yt[:, 0])
t1.train()
# Prediction of the validation points
y1 = t1.predict_values(xtest)

# absolute exponential
t2 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="abs_exp")
t2.set_training_values(xt, yt[:, 0])
t2.train()
# Prediction of the validation points
y2 = t2.predict_values(xtest)

# matern32
t3 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="matern32")
t3.set_training_values(xt, yt[:, 0])
t3.train()
# Prediction of the validation points
y3 = t3.predict_values(xtest)

# matern52
t4 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="matern52")
t4.set_training_values(xt, yt[:, 0])
t4.train()
# Prediction of the validation points
y4 = t4.predict_values(xtest)


print("\n")
print("Comparison of errors")
print("Kriging squared exponential,  err: " + str(compute_rms_error(t1, xtest, ytest)))
print("Kriging absolute exponential,  err: " + str(compute_rms_error(t2, xtest, ytest)))
print("Kriging matern32,  err: " + str(compute_rms_error(t3, xtest, ytest)))
print("Kriging matern52,  err: " + str(compute_rms_error(t4, xtest, ytest)))

"""<div class="alert alert-info fade in" id="d110">
<p>Following the same idea, we can compare different  regression terms</p>
<ol> -  constant term  (by default)  </ol>
<ol> -  linear term  </ol>    
<ol> -  quadratic term </ol>
</div>
"""

# squared exponential + constant term by default
t1 = KRG(
    theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp", poly="constant"
)
t1.set_training_values(xt, yt[:, 0])
t1.train()
# Prediction of the validation points
y1 = t1.predict_values(xtest)

# squared exponential + linear term
t2 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp", poly="linear")
t2.set_training_values(xt, yt[:, 0])
t2.train()
# Prediction of the validation points
y2 = t2.predict_values(xtest)

# squared exponential + quadratic term
t2 = KRG(
    theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp", poly="quadratic"
)
t2.set_training_values(xt, yt[:, 0])
t2.train()
# Prediction of the validation points
y2 = t2.predict_values(xtest)


print("\n")
print("Comparison of errors")
print(
    "Kriging squared exponential + constant term,  err: "
    + str(compute_rms_error(t1, xtest, ytest))
)
print(
    "Kriging squared exponential + linear term,  err: "
    + str(compute_rms_error(t2, xtest, ytest))
)
print(
    "Kriging squared exponential + quadratic term,  err: "
    + str(compute_rms_error(t3, xtest, ytest))
)

"""## Visualization
Here we visualize the prediction using the Kriging surrogate. We can also plot the confidence interval using the estimated variance.
"""

# Plot the surrogate model in 3D
x = np.linspace(-2, 2, 50)
resSM = []
varSM = []
for x0 in x:
    for x1 in x:
        resSM.append(t.predict_values(np.array([[x0, x1]])))
        varSM.append(t.predict_variances(np.array([[x0, x1]])))

resSM = np.array(resSM)
resSM = resSM.reshape((50, 50)).T
varSM = np.array(varSM)
varSM = varSM.reshape((50, 50)).T
X, Y = np.meshgrid(x, x)


fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
ax.scatter(xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="DOE")
surf = ax.plot_surface(
    X, Y, resSM, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.5
)
ax.scatter(
    xtest[:, 0],
    xtest[:, 1],
    ytest,
    zdir="z",
    marker=".",
    c="g",
    s=100,
    label="Validation",
)
ax.scatter(
    xtest[:, 0], xtest[:, 1], y, zdir="z", marker="x", c="r", s=100, label="Prediction"
)
plt.legend()
plt.title("Rosenbrock function with the DOE points and predicted values")

plt.savefig('Rosenbrock function with the DOE points and predicted values.png')

# Plot the surrogate with 99% confidence by using the estimated variance information
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
surf = ax.plot_surface(
    X, Y, resSM, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
)
surf = ax.plot_surface(
    X,
    Y,
    resSM + 3 * np.sqrt(varSM),
    color="r",
    cmap=cm.cool,
    linewidth=0,
    antialiased=False,
    alpha=0.2,
)
surf = ax.plot_surface(
    X,
    Y,
    resSM - 3 * np.sqrt(varSM),
    color="r",
    cmap=cm.cool,
    linewidth=0,
    antialiased=False,
    alpha=0.2,
)


ax.scatter(
    xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="Training point"
)
# ax.scatter(xtest[:,0],xtest[:,1],ytest,zdir='z',marker = '.',c='k',s=200,label='Validation point')

plt.title(" Rosenbrock Surrogate Model with the 99% confidence interval ")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('Rosenbrock Surrogate Model with the 99% confidence interva.png')

# Plot of the variance
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
surf = ax.plot_surface(
    X, Y, varSM, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
)

plt.title("Rosenbrock surrogate model error")
plt.xlabel("x1")
plt.ylabel("x2")
plt.savefig('Rosenbrock surrogate model error.png')
