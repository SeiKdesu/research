# -*- coding: utf-8 -*-
"""GAT_Simplified_Graham,_Jessica_GAE_Clustering_Phase_1_KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/jegraham/1-GNN-Clustering/blob/main/GAT_Simplified_Graham%2C_Jessica_GAE_Clustering_Phase_1_KMeans.ipynb

# Simple K-Means GNN Implementation (Step 1)

This is the initial GNN implementation as referenced in WIDECOMM 2023 Paper Submission. Our implementation uses encoders and decoders with GAT, GCN, and GraphSAGE Layers. Parameters can be modified under the 'Testing Parameters' Section and will be implemented throughout the code.

## Import

### Import Libraries
"""
from datetime import datetime

# 現在の時刻を取得
current_time = datetime.now()
name = f'{current_time}_Spectural'
def visualize_graph(G, color,i,file_dir_name):
    plt.figure(figsize=(3, 3))
    plt.xticks([])
    plt.yticks([])

    # ノードの範囲ごとに縦1列に配置するための位置を設定
    pos = {}

    # 各範囲ごとにノードを縦1列に並べる
    ranges = [[0, 1,2,3,4,5], [ 6, 7, 8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26], [27]]
    x_offset = 0  # X軸のオフセット

    # ノードを正しく配置するためにループを修正
    for r in ranges:
        for i, node in enumerate(r):
            pos[node] = (x_offset, -i)  # Y座標は負の値に設定
        x_offset += 1  # 次の列に移動

    # エッジの重みに基づいて太さを決定
    weights = nx.get_edge_attributes(G, 'weight')
    default_width = 1.0
    edge_widths = [weights[edge] if edge in weights else default_width for edge in G.edges()]

    # グラフを描画
    plt.figure(figsize=(8, 8))
    nx.draw_networkx_nodes(G, pos, node_size=700, node_color=color, cmap=plt.cm.rainbow)
    nx.draw_networkx_labels(G, pos)
    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='gray', arrows=True)

    # 画像を保存
    if(i==1):
        plt.savefig(f'{file_dir_name}/teacher_.png')  # 保存

    plt.savefig(f'{file_dir_name}/predict_.png')  # 保存


import os
import os.path as osp
import shutil
import pandas as pd
import random
import datetime

# libraries for the files in google drive
# from pydrive.auth import GoogleAuth
# from google.colab import drive
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials

import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

# GPU Usage Guide - https://medium.com/@natsunoyuki/speeding-up-model-training-with-google-colab-b1ad7c48573e
if torch.cuda.is_available():
    device_name = torch.device("cuda")
else:
    device_name = torch.device('cpu')
print("Using {}.".format(device_name))


from collections import Counter
import matplotlib.pyplot as plt
import math
import networkx as nx
import numpy as np
from scipy.spatial.distance import cdist, squareform
from scipy import stats
from sklearn.cluster import KMeans, MeanShift, AffinityPropagation, FeatureAgglomeration, SpectralClustering, MiniBatchKMeans, Birch, DBSCAN, OPTICS, AgglomerativeClustering
from sklearn.impute import SimpleImputer
from sklearn.mixture import GaussianMixture
from sklearn.metrics import confusion_matrix, pairwise_distances, davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from torch_geometric.data import Data, InMemoryDataset, download_url
from torch_geometric.loader import DataLoader
from torch_geometric.datasets import Planetoid, TUDataset
import torch_geometric.transforms as T
from torch_geometric.nn import GCNConv, SAGEConv, GAE, GINConv, GATConv
from torch_geometric.utils import train_test_split_edges, to_networkx, from_networkx, to_dense_adj
from torch_geometric.transforms import NormalizeFeatures, ToDevice, RandomLinkSplit, RemoveDuplicatedEdges
import torch.nn.functional as F
from sklearn.metrics import mean_squared_error




"""### Import the Dataset

Process the Data Frame - Modified Code from - https://github.com/jegraham/csv_to_dataframe_to_graph/blob/master/.idea/csv_to_datadrame_conversion.py
"""

# from google.colab import files


"""## Testing Parameters"""

# Define the root directory where the dataset will be stored
root = './'
version = 'v1'
run_id = 'GAT_1000_k_50_dist_150_250_500_transform'

# File Path
folder_path = f'./results/{run_id}_{version}/'
os.makedirs(folder_path, exist_ok=True)


# Define the Number of Clusters
num_clusters = 3

K = num_clusters
clusters = []

# num_Infrastructure = 10 #The number of RSU and Towers in the Dataset (always at the start of the dataset)
# max_dist_tower = 500 #V2I
# max_dist_rsu = 250 #V2R
# max_dist = 150 #V2V

# Channel Parameters & GAE MODEL
in_channels = 6
hidden_channels = 20
out_channels = 1

# Transform Parameters
transform_set = True

# Optimizer Parameters (learning rate)
learn_rate = 0.001

# Epochs or the number of generation/iterations of the training dataset
# epoch and n_init refers to the number of times the clustering algorithm will run different initializations
epochs = 20000
n = 1000
count_0 = [0]*6
count_1 = [0]*6
count_2 = [0]*6



from torch_geometric.nn.conv.gcn_conv import GCNConv

class Net(torch.nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        hidden_size = 20
       
        self.conv1=GCNConv(dataset.num_node_features,hidden_size)
        self.conv2=GCNConv(hidden_size,25)
        self.conv3=GCNConv(25,30)
        self.conv4=GCNConv(30,40)
        self.conv5=GCNConv(40,30)
        self.conv6=GCNConv(30,20)
        self.conv7=GCNConv(20,10)

        self.linear=torch.nn.Linear(10,dataset.num_classes)

    def forward(self,data):
        x = data.x
       
        edge_index = data.edge_index

        x = self.conv1(x,edge_index)
        x=F.relu(x)

        x=self.conv2(x,edge_index)
        x=F.relu(x)
        # x=self.conv7(x,edge_index)
        # x=F.relu(x)
        x=self.conv3(x,edge_index)
        x=F.relu(x)
        x=self.conv4(x,edge_index)
        x=F.relu(x)
        x=self.conv5(x,edge_index)
        x=F.relu(x)
        x=self.conv6(x,edge_index)
        x=F.relu(x)
        x=self.conv7(x,edge_index)
        x=F.relu(x)
   
        x=self.linear(x)
        x=F.softmax(x)
        print(x)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
def train(dt):
    model.train()
    optimizer.zero_grad()
    z = model.encode(dt.x, dt.pos_edge_label_index)
    loss = model.recon_loss(z, dt.pos_edge_label_index)
    loss.backward()
    optimizer.step()
    return float(loss)


def test(dt):
    model.eval()
    with torch.no_grad():
      z = model.encode(dt.x, dt.pos_edge_label_index)
  
    return model.test(z, dt.pos_edge_label_index, dt.neg_edge_label_index)
"""# Run GNN

## InMemory Dataset

Convert Dataset to same format as Planetoid - https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html
"""
from tutorial_rbf import *
best_loss= 100000000000000
for com in range(1):
    
    src=[]
    dst=[]
    for j in range(6):
        for i in range(21):
            src.append(j)

    for i in range(6,27):
        src.append(i)

    for j in range(6):
        for i in range(6,27):
            dst.append(i)
    for i in range(21):
        dst.append(27)
    edge_index=torch.tensor([src,dst],dtype=torch.long)


###############################################################################
    tmp =[]
    length_matrix = matrix()
    one = length_matrix[0]


    for i in range(dim):
        tmp.append(one)
        
    weight1 = weight()
    weight1=weight1.squeeze(1)
    weight1 = weight1.tolist()

    tmp.append(weight1)


    tmp  = [item for sublist in tmp for item in sublist]
    params=[]
    for item in tmp:
        params.append(item)
    params = torch.tensor(params)
    edge_attr=params
    np.random.seed(1234)

#########################################################################

    # ファイル名を指定
    file_name = make_file_path()

    # ファイルを読み込み、行ごとにデータを処理
    with open(file_name, 'r') as file:
        lines = file.readlines()

    # 読み込んだデータをリスト形式に変換
    num = get_xt()
    num=num[com]
    num.reshape(6)
    formatted_weight_data = []
    formatted_weight_data = xt_all()
    # for line in lines:
    #     # 改行を削除し、スペース区切りで数値を分割
    #     values = line.strip().split()
    #     # 各数値をfloat型に変換し、リストに格納
    #     formatted_weight_data.append([float(values[i]) for i in range(6)])
    for i in range(dim-1,-1,-1):
        new_array = np.array([num[i],1,1,1,1,1])
        formatted_weight_data = np.vstack([new_array,formatted_weight_data])


        # formatted_weight_data.insert(i, [num[i] , 1,1,1,1,1])
    # formatted_weight_data.insert(1, [-2.20  , 1,1,1,1,1])
    # formatted_weight_data.insert(2, [0.21596364  , 1,1,1,1,1])
    # formatted_weight_data.insert(3, [ 0.70831308    , 1,1,1,1,1])
    # formatted_weight_data.insert(4, [ -3.63318154  , 1,1,1,1,1])
    # formatted_weight_data.insert(5, [-0.62985134, 1,1,1,1,1])
    to_match = np.array([1,1,1,1,1,1])
    formatted_weight_data = np.vstack([formatted_weight_data,to_match])


    num1 = get_yt()
    new_temp= np.array([num1[com][0],1,1,1,1,1])
    formatted_weight_data = np.vstack([formatted_weight_data,new_temp])
    # formatted_weight_data.append([num1[0], 1,1,1,1,1])#yの値
    x=formatted_weight_data

    #x=np.zeros_like(a)
    x=torch.tensor(x,dtype=torch.float)
    #x=torch.tensor([[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[3],[3],[3]],dtype=torch.float)
    y_tmp=[]
    for i in range(3):
        y_tmp.append(0)
    for i in range(3):
        y_tmp.append(1)
    for i in range(22):
        y_tmp.append(2)

    y = torch.tensor(y_tmp)


    """## Graph AutoEncoder GAE

    Graph AutoEncoders GAE &  
    Variational Graph Autoencoders VGAE    

    [Tutorial 6 paper](https://arxiv.org/pdf/1611.07308.png)  
    [Tutorial 6 code](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/autoencoder.py)

    ### Load the data
    """



    dataset=Data(x=x,edge_index=edge_index,edge_attr=edge_attr,y=y,num_classes=3)
    Data.train_mask=np.array([1 for i in range(len(y))])



    data = dataset
    G=to_networkx(dataset, to_undirected=False)
    dir_file = dirs()
    visualize_graph(G,color=dataset.y,i=1,file_dir_name=dir_file)
    # transform = RemoveDuplicatedEdges()
    # data = transform(data)
    print(data)

    transform = RandomLinkSplit(
        num_val=0.05,
        num_test=0.15,
        is_undirected=False,
        split_labels=True,
        add_negative_train_samples=True)

    train_data, val_data, test_data = transform(data)

    # Display Graphs
    # print(f'Number of graphs: {len(dataset)}')
    # print('dataset',dataset) ## dataset is vector with size 1 because we have one graph

    # print(f'Number of features: {dataset.num_features}')
    # print('------------')

    # # Print information for initialization
    # print('data', data)
    # print('train data',train_data)
    # print('valid data', val_data)
    # print('test data', test_data)
    # print('------------')

    # print(data.is_directed())

    """## Build Graph for Visualization

    ### Visualize Entire Data
    """


    G = to_networkx(data)
    G = G.to_directed()
    
    X = data.x[:,[0,1]].cpu().numpy()

    # X = data.x[:,[0,1]].cpu().detach().numpy()
    pos = dict(zip(range(X[:, 0].size), X))


    # Draw the Graph
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.scatter(X[:,0], X[:,1], s=20, color='grey')
    nx.draw_networkx_nodes(G, pos, node_color='black', node_size=20, ax=ax)
    nx.draw_networkx_edges(G, pos, edge_color='grey', ax=ax)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    plt.savefig(f'{folder_path}{run_id}_{version}-initial-graph', format='eps', dpi=300)


    """### Define the Encoder
    Change the Encoder based on the type testing against
    """

    """### Define the Autoencoder


    """

    # Initialize the Model
    # model = GAE(GCNEncoder(in_channels, hidden_channels, out_channels))
    model = Net()
    model = model.to(device)
    train_data = train_data.to(device)
    test_data = test_data.to(device)
    data_ = data.to(device)

    # Inizialize the Optimizer
    # optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)
    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)
    # loss_func = torch.nn.CrossEntropyLoss()
    # loss_func = nn.MSELoss()
    loss_func =nn.MSELoss()
    print(model)



    auc_values=[]
    ap_values =[]

    best_auc = 0.0  # Track the best AUC value
    consecutive_epochs = 0  # Track the number of consecutive epochs with AUC not increasing
    best_ap = 0.0

    import matplotlib.pyplot as plt


    # 各エポックのlossとAUCの値を保存するリスト
    loss_values = []
    auc_values = []
    accuracy=[]
    best_label=[]

    losses=[]
    acces=[]

    for epoch in range(1, epochs + 1):
        if com == 0:
            optimizer.zero_grad()
        # dataset.to('cpu')
        out = model(dataset).max(dim=1)

        pp,ff = QOL()
        print('teacher_input',pp)
        ff_out =objective_function(pp,dim)
        
        # ff_out = ff_out[0]
        pp= pp
        # print('inputs',pp,ff[0])
        # ff_real = objective_function(pp,dim)
        pp = np.array(pp)
        classfication = out[1]
        # classfication = [0,0,2,1,1,2]
        mal_list0=[]
        list0_count=0
        mal_list1=[]
        list1_count =0
        # mal_list2=[]
        # list2_count =dim
     
        for count in range(dim):
            if classfication[count] == 0:
                mal_list0.append(count)
                list0_count += 1
            if classfication[count] == 1:
                mal_list1.append(count)

        mal_list0= np.array(mal_list0)
        mal_list1 = np.array(mal_list1)

        

    
        prediction=[]
        prediction1=[]
        prediction2 = []
        
        
        # if list0_count != dim : 
        #     prediction = t_preditct(input)
        # else:
        #     prediction[0] = 0
        # input2 = pp*mal_list1
        # if list1_count != dim:
        #     prediction1 = t_preditct(input2)
        # else:
        #     prediction1.append(0.0)
        # input3 = pp*mal_list2
        # if list2_count == dim: 
        #     prediction2.append(0.0)
        # else:
        #     prediction2 = t_preditct(input3)
        print(pp[0]-pp[0])
        print(mal_list0,mal_list1)
<<<<<<< HEAD
        
=======

>>>>>>> refs/remotes/origin/main
        all_prediction = objective_function1(pp,mal_list0,mal_list1)

        print('output',ff_out[0],all_prediction[0])
    
        ff_out = np.array(ff_out,dtype = np.float32)
        # ff_out = ff_out.reshape(20,1)

        
        # print(ff.shape,all_prediction.shape)
        # print("Size of ff:", type(dataset.y.size))
        # print(ff)
        # print("Size of all_prediction:", type(all_prediction))

        # loss_mse= mean_squared_error(ff, all_prediction)
        ff_out = torch.tensor(ff_out,requires_grad=True,dtype=float)
        all_prediction = torch.tensor(all_prediction)
        loss = loss_func(ff_out,all_prediction)
        label = [0,0,2,1,1,2]
        label = torch.tensor(label,dtype=float)
        # loss = loss_func(out[1],label)
        # ターゲットの最大値と最小値を使って最大MSEを計算

        y_min=0
        y_max=50000
        mse_max = (y_max - y_min) ** 2

        # MSEを0～1に正規化
        mse_normalized = loss / mse_max
        loss=mse_normalized
        if loss < best_loss:
            best_loss = loss
            best_mal_0 = mal_list0
            best_mal_1 = mal_list1
    
        losses.append(loss)
        
        print(loss)
        loss.backward()

        optimizer.step()
        

        model.eval()

        _,pred = model(dataset).max(dim=1)
        # scheduler.step()
    
        predict=pred.cpu()
        data_y=dataset.y.cpu()
        count=0
        for i in range(6): 
            if predict[i]==data_y[i]:
                count += 1
        acc = count/6
        acces.append(count/6)
        if acc>0.5:
            print('Epoch %d | Loss: %.4f | ACC: %.4f' % (epoch,loss.item(),count/6))
            print("結果：",predict)
            print("真値：",data_y)
        print("結果：",predict)
        # visualize_graph(G,color=predict)
        # リスト内の各テンソルをdetachしてnumpy配列に変換

        losses_np = [los.detach().numpy() for los in losses]
        # acces_np=   [acc.detach().numpy() for acc in acces]
        import matplotlib.pyplot as plt
        plt.figure(figsize=(10, 5))
        plt.plot(losses_np, label='Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')

        plt.title('Loss vs. Epoch')
        plt.legend()
        plt.grid(True)
        plt.savefig('Graph Neural Network')
        plt.close()
print(best_loss,best_mal_0,best_mal_1)
        # plt.figure(figsize=(10, 5))
        # plt.plot(acces, label='Accracy')
        # plt.xlabel('Epoch')
        # plt.ylabel('Accracy')
        # # plt.ylim(0,0.8)
        # plt.title('Accracy GNN')
        # plt.legend()
        # plt.grid(True)
        # plt.savefig('GNN_ACC')
        # plt.close()







        # acc=0
        # # 訓練データでのlossを取得
        # loss = train(train_data)
    
        # loss_values.append(loss)

        # # テストデータでのAUCとAPを取得
        # auc, ap = test(test_data)
        # auc_values.append(auc)
        # ap_values.append(ap)

        # # 各エポックの結果を表示
        # # print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))

        # # 100エポックごとに表示
        # # if (epoch % 100 == 0):
        # print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))
        # model.eval()
        # with torch.no_grad():
        #     z = model.encode(data_.x, data_.edge_index)
        # z = z.cpu().detach().numpy()
        

        
        # gnn_kmeans = KMeans(n_clusters=num_clusters, n_init=n).fit(z)
        # gnn_labels = gnn_kmeans.labels_

        # from sklearn import cluster
        # # SVMの分類器を訓練
        # spkm = cluster.SpectralClustering(n_clusters=num_clusters,affinity="rbf",assign_labels='discretize')
        # res_spkm = spkm.fit(z)
        # gnn_labels = res_spkm.labels_

        # from sklearn.cluster import AffinityPropagation
        # # SVMの分類器を訓練
        # spkm = AffinityPropagation()
        # res_spkm = spkm.fit(z)
        # gnn_labels = res_spkm.labels_

#         from sklearn import cluster
#         # SVMの分類器を訓練
#         spkm = cluster.AgglomerativeClustering(n_clusters=num_clusters,metric='manhattan', linkage='complete')
#         res_spkm = spkm.fit(z)
#         gnn_labels = res_spkm.labels_

#         # from sklearn import cluster
#         # # SVMの分類器を訓練
#         # spkm = cluster.DBSCAN()
#         # res_spkm = spkm.fit(z)
#         # gnn_labels = res_spkm.labels_

#         if best_loss > loss:
#             best_loss = loss
#             best_label=gnn_labels
#             best_epoch=epoch
#         count=0
#         for i in range(6): 
#             if gnn_labels[i]==dataset.y[i]:
#                 count += 1
#         acc=count/6
        
#         accuracy.append(count/6)
#         # print(count/2)
#         print(gnn_labels)
#         if acc > 0.5:
#             print('acc 90%',gnn_labels,epoch,acc)
#             # break
#         # Early stoppingの条件確認
#         if (auc >= (best_auc - 0.01 * best_auc)) and (ap >= (best_ap - 0.01 * best_ap)):
#             if (auc >= 0.8):
#                 best_auc = auc
#                 consecutive_epochs = 0
#             if (ap >= 0.5):
#                 best_ap = ap
#                 consecutive_epochs = 0
#             if (ap >= 0.5) and (auc >= 0.8):
#                 print("AUC and AP Over GOOD value")
#                 print(gnn_labels,epoch)
#                 break
#         else:
#             consecutive_epochs += 1

#         if (consecutive_epochs >= 10):
#             print('Early stopping: AUC and AP have not increased by more than 1% for 10 epochs.')
#             print(gnn_labels,epoch)
#             break
#     # visualize_graph(G,color=best_label,i=0)
#     for i in range(6): 
#         if gnn_labels[i]== 0:
#             print(i)
#             count_0[i] += 1
#         if gnn_labels[i]== 1:
            
#             count_1[i] += 1
#         if gnn_labels[i]== 2:
#             count_2[i] += 1
#     print(epoch,gnn_labels,loss)
#     print(best_label,best_epoch,best_auc,best_loss)
#     print(count_0,count_1,count_2)
#     visualize_graph(G,color=best_label,i=0,file_dir_name=dir_file)
#     # 訓練終了後にlossとAUCをプロット
#     plt.figure()




#     # Lossのプロット
#     plt.subplot(2, 1, 1)
#     plt.plot(range(1, len(loss_values) + 1), loss_values, label='loss')
#     plt.xlabel('Epoch')
#     plt.ylabel('LOss')
#     plt.title('Loss per Epoch')
#     plt.legend()
#     plt.savefig(f'acc_loss/{name}_rbf_loss.png')  # 保存
#     plt.close()

# plt.close()
# # ヒストグラムの作成
# # 要素のインデックス
# indices = np.arange(len(count_0))  # 0, 1, 2, ... の配列を作成
# file_path_count = os.path.join(dir_file,"count.txt")
# with open(file_path_count, "a") as file:
#     file.write(f"{count_0},{count_1},{count_2}\n")
# # 棒グラフの幅
# width = 0.25  

# # 棒グラフの作成
# plt.bar(indices, count_0, width, label='Data 1', color='blue')
# plt.bar(indices + width, count_1, width, label='Data 2', color='orange')
# plt.bar(indices + 2 * width, count_2, width, label='Data 3', color='green')

# # グラフの設定
# plt.xlabel('Index')
# plt.ylabel('Values')
# plt.title('Comparison of Data')
# plt.xticks(indices + width, range(len(count_0)))  # 横軸の目盛りを設定
# #plt.legend(loc='upper right')  # 凡例の表示

# # グラフを表示
# plt.tight_layout()  # レイアウトの調整
# # グラフを表示
# plt.savefig(f'{dir_file}/hist.png')
