# -*- coding: utf-8 -*-
"""PyG_tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YQzPFk0AbfysRyP6vq3fUx6QPqyz-TKu

# PyTorch Geometric

## 1. グラフデータの表現方法を確認
"""

import torch
print(torch.__version__)
from torch_geometric.data import Data

# グラフの例

# エッジ（COO形式，各ノードごとに接続先のノード番号を指定）
src = [0, 1, 1, 2] # ○番のノードが
dst = [1, 0, 2, 1] # △番のノードにつながっている
edge_index = torch.tensor([src, dst], dtype=torch.long)

print(edge_index)

for e in edge_index.T:
    print(f'{e[0]}番のノードが{e[1]}番につながっている')

# ノードの特徴量
x0 = [1, 2]
x1 = [3, 4]
x2 = [5, 6]
x = torch.tensor([x0, x1, x2], dtype=torch.float)

# ラベル
y0 = 1
y1 = 0
y2 = 1
y = torch.tensor([y0, y1, y2], dtype=torch.float)

# 特徴量，ラベル，エッジの情報をひとつにまとめる
data = Data(x=x, y=y, edge_index=edge_index)

# グラフ情報を表示するための関数
def check_graph(data):
    print("グラフ構造:", data)
    print("グラフのキー: ", data.keys)
    print("ノード数:", data.num_nodes)
    print("エッジ数:", data.num_edges)
    print("ノードの特徴量数:", data.num_node_features)
    print("孤立したノードの有無:", data.contains_isolated_nodes())
    print("自己ループの有無:", data.contains_self_loops())
    print("====== ノードの特徴量:x ======")
    print(data['x'])
    print("====== ノードのクラス:y ======")
    print(data['y'])
    print("========= エッジ形状 =========")
    print(data['edge_index'])

check_graph(data)

import networkx as nx
from matplotlib import pyplot as plt
import numpy as np
from torch_geometric.utils import to_networkx

def visualization(data):
    # networkxのグラフに変換
    nxg = to_networkx(data)

    # 可視化する際のノード位置
    draw_pos = nx.spring_layout(nxg, seed=0)

    # ノードの色設定
    cmap = plt.get_cmap('tab10')
    labels = data.y.numpy()
    colors = [cmap(l) for l in labels] # ラベルに応じて色分け

    # 図のサイズ
    plt.figure(figsize=(10, 10))

    # ノードの描画
    nx.draw_networkx_nodes(nxg,
                          draw_pos,
                          node_size=500,
                          node_color=colors, alpha=0.5)
    # エッジの描画
    nx.draw_networkx_edges(nxg, draw_pos, arrowstyle='-', alpha=0.2)

    # ノード番号の描画
    nx.draw_networkx_labels(nxg, draw_pos, font_size=10)

    # 表示
    #plt.show()


visualization(data)

"""## 2. 空手クラブデータの読み込み"""

from torch_geometric.datasets import KarateClub

# 空手クラブデータの読み込み
dataset = KarateClub()

# 空手クラブはひとつのグラフのみ
print("グラフ数:", len(dataset))
print("クラス数:",dataset.num_classes)

data = dataset[0]

check_graph(data)

visualization(data)

"""## 3. GCNモデルの構築"""

# PyTorch
import torch
import torch.nn.functional as F

# PyTorch Geometric
from torch_geometric.nn.conv.gcn_conv import GCNConv # Kiph & Welling の Spatial Conv.

# GCNモデル
class Net(torch.nn.Module):
    # コンストラクタ
    def __init__(self):
        super(Net, self).__init__()
        # 中間層の各ノードの特徴量ベクトルの次元数
        hidden_size = 5

        # 1層目（グラフ畳み込み）
        self.conv1 = GCNConv(dataset.num_node_features, # 入力される特徴ベクトルの次元数
                                hidden_size) # 出力される特徴ベクトルの次元数

        # 2層目（グラフ畳み込み）
        self.conv2 = GCNConv(hidden_size, hidden_size)

        # 3層目（全結合）
        self.linear = torch.nn.Linear(hidden_size,
                                    dataset.num_classes) # クラス数

    # 順伝播処理
    def forward(self, data):
        # 各ノードの特徴ベクトルを取り出す
        x = data.x

        # エッジの情報を取り出す
        edge_index = data.edge_index

        # 1層目
        x = self.conv1(x, edge_index)

        # 活性化関数
        x = F.relu(x)

        # 2層目
        x = self.conv2(x, edge_index)

        # 活性化関数
        x = F.relu(x)

        # 3層目
        x = self.linear(x)

        return x

# デバイス設定
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# モデルのインスタンス生成
model = Net()
print(model)
# GPU利用時にはGPUにパラメータを転送
model.to(device)

# モデルを訓練モードに設定
model.train()

"""## 4. GCNモデルの学習・推論"""

# 最適化アルゴリズム（今回はAdam）
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# 損失関数（分類問題なのでクロスエントロピー損失）
loss_func = torch.nn.CrossEntropyLoss()

# 学習ループ
for epoch in range(100):
    # 勾配を初期化
    optimizer.zero_grad()

    # GPU利用時にはGPUにデータを転送
    data.to(device)

    # 順伝播
    out = model(data)

    # 損失の計算
    loss = loss_func(out, data.y)

    # 逆伝播
    loss.backward()

    # パラメータの更新
    optimizer.step()
    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))

# モデルを評価モードに設定
model.eval()

# 推論
_, pred = model(data).max(dim=1)

print("結果：", pred.cpu())
print("真値：", data.y.cpu())

# 正答率の計算
def accuracy(pred, label):
    total = label.size(0)
    correct = (pred == label).sum().item()
    return correct / total

accuracy(pred, data.y)

"""## 5. [発展] グラフ分類
- 今回使用するのは Mutagenicity データセット
- 化合物の変異原性（=発がん性）を予測する問題
  - ラベル（グラフごとに）
    - 1：変異原性あり
    - 0：変異原性なし
  - 各ノードの特徴量
    - 原子をワンホット表現したもの
  - データ数：4337


from torch_geometric.data import DataLoader
from torch_geometric.datasets import TUDataset

path = '.'
dataset = TUDataset(path, name='Mutagenicity').shuffle() # 読み込んでからデータをシャッフル
test_dataset = dataset[:len(dataset) // 10] # 全体の10%を検証用データとして使用
train_dataset = dataset[len(dataset) // 10:] # 残りの90%を学習用データとして使用

# バッチサイズ 128 として DataLoaderを定義
test_loader = DataLoader(test_dataset, batch_size=128)
train_loader = DataLoader(train_dataset, batch_size=128)

### 分子構造の可視化

# 分子構造の可視化用の関数
def draw_molecule(g, edge_mask=None, draw_edge_labels=False):
    g = g.copy().to_undirected()
    node_labels = {}
    for u, data in g.nodes(data=True):
        node_labels[u] = data['name']
    pos = nx.planar_layout(g)
    pos = nx.spring_layout(g, pos=pos)
    if edge_mask is None:
        edge_color = 'black'
        widths = None
    else:
        edge_color = [edge_mask[(u, v)] for u, v in g.edges()]
        widths = [x * 10 for x in edge_color]
    nx.draw(g, pos=pos, labels=node_labels, width=widths,
            edge_color=edge_color, edge_cmap=plt.cm.Blues,
            node_color='azure')

    if draw_edge_labels and edge_mask is not None:
        edge_labels = {k: ('%.2f' % v) for k, v in edge_mask.items()}
        nx.draw_networkx_edge_labels(g, pos, edge_labels=edge_labels,
                                    font_color='red')
    plt.show()


def to_molecule(data):
    ATOM_MAP = ['C', 'O', 'Cl', 'H', 'N', 'F',
                'Br', 'S', 'P', 'I', 'Na', 'K', 'Li', 'Ca']
    g = to_networkx(data, node_attrs=['x'])
    for u, data in g.nodes(data=True):
        data['name'] = ATOM_MAP[data['x'].index(1.0)]
        del data['x']
    return g

# ランダムに分子構造を表示（実行するたびに表示が変化）
import random
data = random.choice([t for t in train_dataset])
mol = to_molecule(data)
plt.figure(figsize=(10, 5))
draw_molecule(mol) # 画像の表示
check_graph(data) # グラフ情報の表示

### GCNモデルの構築

import torch
import torch.nn.functional as F
from torch.nn import Linear

from torch_geometric.nn import global_mean_pool, GraphConv

class GraphNet(torch.nn.Module):
    def __init__(self, dim):
        super(GraphNet, self).__init__()

        # 入力される特徴ベクトルの次元数
        num_features = dataset.num_features

        # 中間層の特徴ベクトルの次元数
        self.dim = dim

        # グラフ畳み込み 5層分
        self.conv1 = GCNConv(num_features, dim)
        self.conv2 = GCNConv(dim, dim)
        self.conv3 = GCNConv(dim, dim)
        self.conv4 = GCNConv(dim, dim)
        self.conv5 = GCNConv(dim, dim)

        # 全結合層
        self.fc1 = Linear(dim, dim)

        # 最終層
        self.fc2 = Linear(dim,
                          dataset.num_classes) # クラス数(=2)

    # 順伝播処理
    def forward(self, x, edge_index, batch):
        # グラフ畳み込み→ReLU を 5層分
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = F.relu(self.conv3(x, edge_index))
        x = F.relu(self.conv4(x, edge_index))
        x = F.relu(self.conv5(x, edge_index))

        # グラフごとに全ノードの特徴ベクトルを平均化
        x = global_mean_pool(x,
                             batch) # バッチ情報（どの特徴ベクトルがどのグラフのものか）

        # 全結合層→ReLU
        x = F.relu(self.fc1(x))

        # 全結合層
        x = self.fc2(x)

        return x

### GCNモデルの学習・推論

# 学習用関数
def train():
    # モデルを学習モードへ
    model.train()

    loss_all = 0

    # 損失関数（分類問題なのでクロスエントロピー損失）
    loss_func = torch.nn.CrossEntropyLoss()

    # 学習ループ（1epoch分）
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        output = model(data.x, data.edge_index, data.batch)
        loss = loss_func(output, data.y)
        loss.backward()
        loss_all += loss.item() * data.num_graphs
        optimizer.step()

    # 1epochの平均損失
    return loss_all / len(train_dataset)

# 評価用関数
def test(loader):
    # モデルを評価モードへ
    model.eval()

    # 正答数を計算
    correct = 0
    for data in loader:
        data = data.to(device)
        output = model(data.x, data.edge_index, data.batch)
        pred = output.max(dim=1)[1]
        correct += pred.eq(data.y).sum().item()

    # 正答率を計算
    return correct / len(loader.dataset)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GraphNet(dim=32).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 今回は20epoch実施
for epoch in range(20):
    # 1epoch分学習を実行
    train_loss = train()

    # 学習用データに対する正答率を計算
    train_acc = test(train_loader)

    # テストデータに対する正答率を計算
    test_acc = test(test_loader)
    print('Epoch: {:03d}, Train Loss: {:.7f}, '
          'Train Acc: {:.7f}, Test Acc: {:.7f}'.format(epoch+1, train_loss,
                                                  train_acc, test_acc))

## [演習1]
* モデルの層の数を変更してみよう

## [演習2]
* グラフデータのデータセットの例を調べてみよう
- https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html
  

"""

