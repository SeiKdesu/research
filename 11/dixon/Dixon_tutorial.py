# -*- coding: utf-8 -*-
"""SMT_Tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Qy6YHWJYx2osAdr0Bv3b4XgX2Nr57IT

<a href="https://colab.research.google.com/github/SMTorg/smt/blob/master/tutorial/SMT_Tutorial.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

<div class="jumbotron text-left"><b>
This tutorial describes how to use the SMT toolbox, which is a python toolbox for building a surrogate model.</b></div>

Nathalie BARTOLI ONERA/DTIS/M2CI - May 2024

based on `SMT 2.5.1 version`

<div class="alert alert-info fade in" id="d110">
<p>Latest updates</p>
<ol> - sparse gaussian process for large database </ol>
<ol> - use GPX build in Rust in order to reduce time for trining and prediction (KRG or KPLS models) </ol>
    
 <p>Previous updates</p>  
<ol>     -correct the figures with matplotlib:  ax =  fig.add_subplot(projection='3d') </ol>
<ol> - prediction and variance derivatives for different kernels (squared exponential, absolute exponential, matern 32, matern 52) and trends (constant, linear) </ol>
<ol> -  Kriging with integer variables </ol>   
<ol> -  compare different covariance kernels for the kriging models </ol>
<ol> -  automatic choice of PLS comonents </ol>
<ol> -  example to use Marginal Gaussian Process   </ol>    
<ol> -  example to use multiple outputs (independant outputs) </ol>
<ol> -  example to compare models using Mixture of experts technique  </ol>  
<ol> -  example to save and load models using  `pickle`  </ol>    

</div>

<p class="alert alert-success" style="padding:1em">
To use SMT models, please follow this link : https://github.com/SMTorg/SMT/blob/master/README.md. The documentation is available here: http://smt.readthedocs.io/en/latest/
</p>

The reference paper is available
here https://www.sciencedirect.com/science/article/pii/S0965997818309360?via%3Dihub

or as a preprint: https://www.researchgate.net/profile/Mohamed_Amine_Bouhlel/publication/331976718_A_Python_surrogate_modeling_framework_with_derivatives/links/5cc3cebd299bf12097829631/A-Python-surrogate-modeling-framework-with-derivatives.pdf

Cite us:

M.-A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Morlier, J .R.R.A Martins (2019), A Python surrogate modeling framework with derivatives, Advances in Engineering Software, 102662

P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, J. RRA. Martins (2024). SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes. Advances in Engineering Software, 188, 103571.

@article{SMT2019,
  title={A Python surrogate modeling framework with derivatives},
  author={Mohamed Amine Bouhlel and John T. Hwang and Nathalie Bartoli and R{\'e}mi Lafage and Joseph Morlier and Joaquim R. R. A. Martins},
  journal={Advances in Engineering Software},
  pages={102662},
  year={2019},
  publisher={Elsevier}
}

@article{saves2024smt,
  title={SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes},
  author={Saves, Paul and Lafage, R{\'e}mi and Bartoli, Nathalie and Diouane, Youssef and Bussemaker, Jasper and Lefebvre, Thierry and Hwang, John T and Morlier, Joseph and Martins, Joaquim RRA},
  journal={Advances in Engineering Software},
  volume={188},
  pages={103571},
  year={2024},
  publisher={Elsevier}
}

<div class="alert alert-danger" >
<p> In most of the surrogate models $y\in\mathbb{R}$, so if you have multiple outputs $y\in\mathbb{R}^d$ (which are considered as independant outputs), add an external loop to build one surrogate model per output. The correlations betweens outputs are not taken into account. Section 11 of this notebook presents an example with 2 outputs.
</div>

<div class="alert alert-warning" >
In this notebook, the database is considered to be noise-free. If you want to consider noisy data, please have a look at the dedicated notebook available within SMT.
</div>
"""

from __future__ import print_function, division

# !pip install smt
# # to reduce CPU time
# !pip install numba

"""<div class="alert alert-warning" >
If you use hierarchical variables and the size of your doe greater than 30 points, you may leverage the `numba` JIT compiler to speed up the computation
To do so:
    
 - install numba library
    
     `pip install numba`
    
    
 - and define the environment variable `USE_NUMBA_JIT = 1` (unset or 0 if you do not want to use numba)
    
     - Linux: export USE_NUMBA_JIT = 1
    
     - Windows: set USE_NUMBA_JIT = 1

</div>
"""

# # to check if numba is available
# !pip show numba
# # and then you need to define the environment variable USE_NUMBA_JIT
# !echo "Numba used or not in your environment=" %USE_NUMBA_JIT%

"""# 0. Construction of the DOE points"""

from smt.problems.problem import Problem

import numpy as np
from smt.problems.problem import Problem
import numpy as np
from smt.problems.problem import Problem

import numpy as np
from smt.problems.problem import Problem

class DixonPrice(Problem):
    def _initialize(self):
        self.options.declare("name", "DixonPrice", types=str)

    def _setup(self):
        self.xlimits[:, 0] = -10.0
        self.xlimits[:, 1] = 10.0

    def _evaluate(self, x, kx):
        """
        Arguments
        ---------
        x : ndarray[ne, nx]
            Evaluation points.
        kx : int or None
            Index of derivative (0-based) to return values with respect to.
            None means return function value rather than derivative.

        Returns
        -------
        ndarray[ne, 1]
            Functions values if kx=None or derivative values if kx is an int.
        """
        ne, nx = x.shape

        y = np.zeros((ne, 1))
        if kx is None:
            # Evaluate the function value
            term1 = (x[:, 0] - 1)**2
            sum_terms = np.zeros(ne)
            for i in range(1, nx):
                xi = x[:, i]
                xi_1 = x[:, i - 1]
                sum_terms += i * (2 * xi**2 - 1)**2
            y[:, 0] = term1 + sum_terms
        else:
            if kx == 0:
                # Derivative with respect to x[0]
                y[:, 0] = 2 * (x[:, 0] - 1)
            elif 1 <= kx < nx:
                # Derivative with respect to x[kx]
                xi = x[:, kx]
                term1 = 4 * (2 * xi**2 - 1) * (2 * xi)
                y[:, 0] = i * term1
            else:
                raise ValueError("kx is out of range")

        return y



import numpy as np
from smt.utils.misc import compute_rms_error

# from smt.problems import DixonPrice
from smt.sampling_methods import LHS
from smt.surrogate_models import LS, QP, KPLS, KRG, KPLSK, GEKPLS, MGP

try:
    from smt.surrogate_models import IDW, RBF, RMTC, RMTB

    compiled_available = True
except Exception:
    compiled_available = False

try:
    import matplotlib.pyplot as plt

    plot_status = True
except Exception:
    plot_status = False


import matplotlib.pyplot as plt
from matplotlib import cm


# to ignore warning messages
import warnings

warnings.filterwarnings("ignore")

"""## DixonPrice Function  in dimension N

$$
f(\mathbf{x}) = \sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2 \quad \mbox{where} \quad \mathbf{x} = [x_1, \ldots, x_N] \in \mathbb{R}^N.
$$

$$x_i \in [-2,2]$$

## Training points and validation points using a LHS algorithm

Here we outline the studied function by plotting the surface representing the function. In addition, we plot also the training points used later to build the surrogate model as well as the validation points which will be used to evaluate the quality of the surrogate model. Both the training point and validation points are generated by a LHS DOE algorithm.

<div class="alert alert-danger" >
<p> In order to have reproducibility of the tests and results, we use  the option random_state to set a seed to the random generator, so that your DOE points are always deterministic. If you don't set a seed, it is different each time.
</div>
"""

########### Initialization of the problem, construction of the training and validation points

ndim = 2
ndoe = 20  # int(10*ndim)
# Define the function
fun = DixonPrice(ndim=ndim)

# Construction of the DOE
# in order to have the always same LHS points, random_state=1
sampling = LHS(xlimits=fun.xlimits, criterion="ese", random_state=1)
xt = sampling(ndoe)
# Compute the outputs
yt = fun(xt)

# Construction of the validation points
ntest = 200  # 500
sampling = LHS(xlimits=fun.xlimits, criterion="ese", random_state=1)
xtest = sampling(ntest)
ytest = fun(xtest)

# To visualize the DOE points
fig = plt.figure(figsize=(10, 10))
plt.scatter(xt[:, 0], xt[:, 1], marker="x", c="b", s=200, label="Training points")
plt.scatter(
    xtest[:, 0], xtest[:, 1], marker=".", c="k", s=200, label="Validation points"
)
plt.title("DOE")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('DOE.png')
plt.close()

# To plot the DixonPrice function
x = np.linspace(-2, 2, 50)
res = []
for x0 in x:
    for x1 in x:
        res.append(fun(np.array([[x0, x1]])))
res = np.array(res)
res = res.reshape((50, 50)).T
X, Y = np.meshgrid(x, x)
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
surf = ax.plot_surface(
    X, Y, res, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
)

ax.scatter(
    xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="Training point"
)
ax.scatter(
    xtest[:, 0],
    xtest[:, 1],
    ytest,
    zdir="z",
    marker=".",
    c="k",
    s=200,
    label="Validation point",
)

plt.title("DixonPrice function")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('DixonPrice function.png')
plt.close()


# 等高線を描画
plt.contour(  X, Y, res, levels=30, cmap='viridis')

# カラーバーの追加
plt.colorbar()

# グラフの表示
plt.savefig('DixonPrice_contour.pdf')
plt.close()
"""Different models will be used and compared:

- Linear Model (first order polynomial)
- Quadratic Model (second order polynomial)
- Kriging Model (also known as Gaussian Process)
- KPLS Model and KPLSK Model (useful for Kriging in high dimension)
- IDW Model (Inverse Distance Weighting)
- RBF Model (Radial Basis Function)
- RMTS Models: RMTB and RMTC (useful for low dimensional problem)

Some metrics to assess some errors are proposed at the end.

Mixture of experts technique will be used to compare different surrogate models.

# 1. Linear Model
"""

########### The LS model

# Initialization of the model
t = LS(print_prediction=False)

# Add the DOE
t.set_training_values(xt, yt[:, 0])

# Train the model
t.train()

# Prediction of the validation points
y = t.predict_values(xtest)
print("LS,  err: " + str(compute_rms_error(t, xtest, ytest)))

# Plot prediction/true values
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("LS model: validation of the prediction model")
    plt.savefig('LS model: validation of the prediction model.png')
    plt.close()
    # 等高線を描画
    x = np.linspace(-2, 2, 50)
    res = []
    for x0 in x:
        for x1 in x:
            res.append(t.predict_values(np.array([[x0, x1]])))
    res = np.array(res)
    res = res.reshape((50, 50)).T
    X, Y = np.meshgrid(x, x)
    plt.contour(  X, Y, res, levels=30, cmap='viridis')

    # カラーバーの追加
    plt.colorbar()

    # グラフの表示
    plt.savefig('Linear_DixonPrice_contour.pdf')
    plt.close()
   

"""# 2. Quadratic Model"""

########### The QP model

t = QP(print_prediction=False)
t.set_training_values(xt, yt[:, 0])

t.train()

# Prediction of the validation points
y = t.predict_values(xtest)
print("QP,  err: " + str(compute_rms_error(t, xtest, ytest)))

# Plot prediction/true values
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("QP model: validation of the prediction model")
    plt.savefig('QP model: validation of the prediction model.png')
    plt.close()
    # 等高線を描画
    x = np.linspace(-2, 2, 50)
    res = []
    for x0 in x:
        for x1 in x:
            res.append(t.predict_values(np.array([[x0, x1]])))
    res = np.array(res)
    res = res.reshape((50, 50)).T
    X, Y = np.meshgrid(x, x)
    plt.contour(  X, Y, res, levels=30, cmap='viridis')

    # カラーバーの追加
    plt.colorbar()

    # グラフの表示
    plt.savefig('Quadric_DixonPrice_contour.pdf')
    plt.close()

"""# 3. Kriging Model"""

########### The Kriging model

# The variable 'theta0' is a list of length ndim.
t = KRG(theta0=[1e-2] * ndim, print_prediction=False)
t.set_training_values(xt, yt[:, 0])

t.train()

# Prediction of the validation points
y = t.predict_values(xtest)
print("Kriging,  err: " + str(compute_rms_error(t, xtest, ytest)))
if plot_status:
    # Plot the function and the prediction
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("Kriging model: validation of the prediction model")
    plt.close()
    # 等高線を描画
    x = np.linspace(-2, 2, 50)
    res = []
    for x0 in x:
        for x1 in x:
            res.append(t.predict_values(np.array([[x0, x1]])))
    res = np.array(res)
    res = res.reshape((50, 50)).T
    X, Y = np.meshgrid(x, x)
    plt.contour(  X, Y, res, levels=30, cmap='viridis')

    # カラーバーの追加
    plt.colorbar()

    # グラフの表示
    plt.savefig('Kring_DixonPrice_contour.png')
    plt.close()
    

if plot_status:
    plt.savefig('Kriging model: validation of the prediction model.png')

# Value of theta
print("theta values", t.optimal_theta)

"""Associated to the Kriging prediction, there is the estimated variance in order to have a confidence interval.

"""

# estimated variance for the validation points
s2 = t.predict_variances(xtest)
# plot with the associated interval confidence
yerr = (
    2 * 3 * np.sqrt(s2)
)  # in order to use +/- 3 x standard deviation: 99% confidence interval estimation
if plot_status:
    # Plot the function, the prediction and the 99% confidence interval based on
    # the MSE
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")
    plt.errorbar(
        np.squeeze(ytest),
        np.squeeze(y),
        yerr=np.squeeze(yerr),
        fmt="none",
        capsize=5,
        ecolor="lightgray",
        elinewidth=1,
        capthick=0.5,
        label="confidence estimate 99%",
    )
    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title(
        "Kriging model: validation of the prediction model with the estimate of confidence"
    )

if plot_status:
    plt.savefig('Kriging model: validation of the prediction model with the estimate of confidence.png')

"""You can also have access to the kriging derivative prediction

"""

# estimated derivative for the validation points
dydx1 = t.predict_derivatives(xtest, 0)  # derivative according to the x1
dydx2 = t.predict_derivatives(xtest, 1)  # derivative according to the x2

# estimated variance derivative for the validation points
dsigmadx1 = t.predict_variance_derivatives(    xtest, 0)  # variance derivative according to the x1
dsigmadx2 = t.predict_variance_derivatives(
    xtest, 1
)  # variance derivative according to the x2

"""<div class="alert alert-info fade in" id="d110">
<p>The idea could be to compare different covariance kernels</p>
<ol> -  a squared exponential kernel (by default)  </ol>
<ol> -  an absolute exponential kernel   </ol>    
<ol> - some Matern Kernels (matern 32 and matern 52) </ol>
</div>
"""

# squared exponential by default
t1 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp")
t1.set_training_values(xt, yt[:, 0])
t1.train()
# Prediction of the validation points
y1 = t1.predict_values(xtest)

# absolute exponential
t2 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="abs_exp")
t2.set_training_values(xt, yt[:, 0])
t2.train()
# Prediction of the validation points
y2 = t2.predict_values(xtest)

# matern32
t3 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="matern32")
t3.set_training_values(xt, yt[:, 0])
t3.train()
# Prediction of the validation points
y3 = t3.predict_values(xtest)

# matern52
t4 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="matern52")
t4.set_training_values(xt, yt[:, 0])
t4.train()
# Prediction of the validation points
y4 = t4.predict_values(xtest)


print("\n")
print("Comparison of errors")
print("Kriging squared exponential,  err: " + str(compute_rms_error(t1, xtest, ytest)))
print("Kriging absolute exponential,  err: " + str(compute_rms_error(t2, xtest, ytest)))
print("Kriging matern32,  err: " + str(compute_rms_error(t3, xtest, ytest)))
print("Kriging matern52,  err: " + str(compute_rms_error(t4, xtest, ytest)))

"""<div class="alert alert-info fade in" id="d110">
<p>Following the same idea, we can compare different  regression terms</p>
<ol> -  constant term  (by default)  </ol>
<ol> -  linear term  </ol>    
<ol> -  quadratic term </ol>
</div>
"""

# squared exponential + constant term by default
t1 = KRG(
    theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp", poly="constant"
)
t1.set_training_values(xt, yt[:, 0])
t1.train()
# Prediction of the validation points
y1 = t1.predict_values(xtest)

# squared exponential + linear term
t2 = KRG(theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp", poly="linear")
t2.set_training_values(xt, yt[:, 0])
t2.train()
# Prediction of the validation points
y2 = t2.predict_values(xtest)

# squared exponential + quadratic term
t2 = KRG(
    theta0=[1e-2] * ndim, print_prediction=False, corr="squar_exp", poly="quadratic"
)
t2.set_training_values(xt, yt[:, 0])
t2.train()
# Prediction of the validation points
y2 = t2.predict_values(xtest)


print("\n")
print("Comparison of errors")
print(
    "Kriging squared exponential + constant term,  err: "
    + str(compute_rms_error(t1, xtest, ytest))
)
print(
    "Kriging squared exponential + linear term,  err: "
    + str(compute_rms_error(t2, xtest, ytest))
)
print(
    "Kriging squared exponential + quadratic term,  err: "
    + str(compute_rms_error(t3, xtest, ytest))
)

"""## Visualization
Here we visualize the prediction using the Kriging surrogate. We can also plot the confidence interval using the estimated variance.
"""

# Plot the surrogate model in 3D
x = np.linspace(-2, 2, 50)
resSM = []
varSM = []
for x0 in x:
    for x1 in x:
        resSM.append(t.predict_values(np.array([[x0, x1]])))
        varSM.append(t.predict_variances(np.array([[x0, x1]])))

resSM = np.array(resSM)
resSM = resSM.reshape((50, 50)).T
varSM = np.array(varSM)
varSM = varSM.reshape((50, 50)).T
X, Y = np.meshgrid(x, x)


fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
ax.scatter(xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="DOE")
surf = ax.plot_surface(
    X, Y, resSM, cmap=cm.coolwarm, linewidth=0, antialiased=False, alpha=0.5
)
ax.scatter(
    xtest[:, 0],
    xtest[:, 1],
    ytest,
    zdir="z",
    marker=".",
    c="g",
    s=100,
    label="Validation",
)
ax.scatter(
    xtest[:, 0], xtest[:, 1], y, zdir="z", marker="x", c="r", s=100, label="Prediction"
)
plt.legend()
plt.title("DixonPrice function with the DOE points and predicted values")

plt.savefig('DixonPrice function with the DOE points and predicted values.png')

# Plot the surrogate with 99% confidence by using the estimated variance information
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
surf = ax.plot_surface(
    X, Y, resSM, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
)
surf = ax.plot_surface(
    X,
    Y,
    resSM + 3 * np.sqrt(varSM),
    color="r",
    cmap=cm.cool,
    linewidth=0,
    antialiased=False,
    alpha=0.2,
)
surf = ax.plot_surface(
    X,
    Y,
    resSM - 3 * np.sqrt(varSM),
    color="r",
    cmap=cm.cool,
    linewidth=0,
    antialiased=False,
    alpha=0.2,
)


ax.scatter(
    xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="Training point"
)
# ax.scatter(xtest[:,0],xtest[:,1],ytest,zdir='z',marker = '.',c='k',s=200,label='Validation point')

plt.title(" DixonPrice Surrogate Model with the 99% confidence interval ")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('DixonPrice Surrogate Model with the 99% confidence interva.png')

# Plot of the variance
fig = plt.figure(figsize=(15, 10))
ax = fig.add_subplot(projection="3d")
surf = ax.plot_surface(
    X, Y, varSM, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
)

plt.title("DixonPrice surrogate model error")
plt.xlabel("x1")
plt.ylabel("x2")
plt.savefig('DixonPrice surrogate model error.png')

"""# 4. KPLS and KPLSK Model

Here we implement the KPLS and KPLSK models (useful for high dimensional input space, approximatively greater than 10).

<div class="alert alert-warning">
<p>Only some covariance kernels are available for these models </p>
<ol> -  a squared exponential or an absolute exponential kernel for KPLS </ol>
<ol> -  a squared exponential  kernel for KPLSK </ol>    
</div>

## 4.1 KPLS Model
"""

########### The KPLS model

# 'n_comp' and 'theta0' must be an integer in [1,ndim[ and a list of length n_comp, respectively.
n_comp = 1
t = KPLS(n_comp=n_comp, theta0=n_comp * [1e-2], print_prediction=False, corr="abs_exp")
t.set_training_values(xt, yt[:, 0])

t.train()

# Prediction of the validation points
y = t.predict_values(xtest)
print("KPLS,  err: " + str(compute_rms_error(t, xtest, ytest)))

# plot prediction/true values
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("KPLS model: validation of the prediction model")

if plot_status:
    plt.savefig('KPLS model: validation of the prediction model.png')

# estimated variance for the validation points
s2 = t.predict_variances(xtest)
# plot with the associated interval confidence
yerr = (
    2 * 3 * np.sqrt(s2)
)  # in order to use +/- 3 x standard deviation: 99% confidence interval estimation
if plot_status:
    # Plot the function, the prediction and the 99% confidence interval based on
    # the MSE
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")
    plt.errorbar(
        np.squeeze(ytest),
        np.squeeze(y),
        yerr=np.squeeze(yerr),
        fmt="none",
        capsize=5,
        ecolor="lightgray",
        elinewidth=1,
        capthick=0.5,
        label="confidence estimate 99%",
    )
    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title(
        "KPLS model: validation of the prediction model with the estimate of confidence"
    )

if plot_status:
    plt.savefig('KPLS model: validation of the prediction model with the estimate of confidence.png')

"""## 4.2 Automatic selection of the number of components

"""

# 'n_comp' and 'theta0' must be an integer in [1,ndim[ and a list of length n_comp, respectively.
# 'eval_n_comp'  must be a boolean.

t = KPLS(eval_n_comp=True, print_prediction=False, corr="abs_exp")
t.set_training_values(xt, yt[:, 0])

t.train()
l_comp = t.options["n_comp"]
print(
    "\n --------------------------\n",
    l_comp,
    "COMPONENTS FOUND OPTIMAL\n --------------------------\n",
)
# Prediction of the validation points
y = t.predict_values(xtest)
print("KPLS,  err: " + str(compute_rms_error(t, xtest, ytest)))

# plot prediction/true values
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("KPLS model: validation of the prediction model")

if plot_status:
    plt.savefig('KPLS model: validation of the prediction model.png')

"""## 4.3 KPLSK Model"""

########### The KPLSK model

# 'n_comp' and 'theta0' must be an integer in [1,ndim[ and a list of length n_comp, respectively.

t = KPLSK(n_comp=2, theta0=[1e-2, 1e-2], print_prediction=False)
t.set_training_values(xt, yt[:, 0])

t.train()

# Prediction of the validation points
y = t.predict_values(xtest)
print("KPLSK,  err: " + str(compute_rms_error(t, xtest, ytest)))

# plot prediction/true values
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("KPLSK model: validation of the prediction model")

if plot_status:
    plt.savefig('KPLSK model: validation of the prediction model.png')

# estimated variance for the validation points
s2 = t.predict_variances(xtest)
# plot with the associated interval confidence
yerr = (
    2 * 3 * np.sqrt(s2)
)  # in order to use +/- 3 x standard deviation: 99% confidence interval estimation
if plot_status:
    # Plot the function, the prediction and the 99% confidence interval based on
    # the MSE
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")
    plt.errorbar(
        np.squeeze(ytest),
        np.squeeze(y),
        yerr=np.squeeze(yerr),
        fmt="none",
        capsize=5,
        ecolor="lightgray",
        elinewidth=1,
        capthick=0.5,
        label="confidence estimate 99%",
    )
    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title(
        "KPLSK model: validation of the prediction model with the estimate of confidence"
    )

if plot_status:
    plt.savefig('KPLSK model: validation of the prediction model with the estimate of confidence.png')

"""# 5. IDW Model
Here we implement the IDW model.
"""

if compiled_available:
    ########### The IDW model

    t = IDW(print_prediction=False)
    t.set_training_values(xt, yt[:, 0])

    t.train()

    # Prediction of the validation points
    y = t.predict_values(xtest)
    print("IDW,  err: " + str(compute_rms_error(t, xtest, ytest)))
    if plot_status:
        plt.figure()
        plt.plot(ytest, ytest, "-.")
        plt.plot(ytest, y, ".")
        plt.xlabel(r"$y_{true}$")
        plt.ylabel(r"$\hat{y}$")
        plt.title("Validation of the IDW model")
        plt.savefig('Validation of the IDW model.png')

"""# 6. RBF Model
Here we implement the RBF model.
"""

if compiled_available:
    ########### The RBF model

    t = RBF(print_prediction=False, poly_degree=0)
    t.set_training_values(xt, yt[:, 0])

    t.train()

    # Prediction of the validation points
    y = t.predict_values(xtest)
    print("RBF,  err: " + str(compute_rms_error(t, xtest, ytest)))
    # Plot prediction/true values
    if plot_status:
        fig = plt.figure()
        plt.plot(ytest, ytest, "-", label="$y_{true}$")
        plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

        plt.xlabel("$y_{true}$")
        plt.ylabel(r"$\hat{y}$")

        plt.legend(loc="upper left")
        plt.title("RBF model: validation of the prediction model")
        plt.savefig('RBF model: validation of the prediction model.png')
        plt.close()
    # 等高線を描画
    print('描画します')
    x_rbf = np.linspace(-2, 2, 50)
    res_rbf = []
    for x0 in x_rbf:
        for x1 in x_rbf:
            res_rbf.append(t.predict_values(np.array([[x0, x1]])))
    res_rbf = np.array(res_rbf)
    res_rbf = res_rbf.reshape((50, 50)).T
    X, Y = np.meshgrid(x_rbf, x_rbf)
    
    # Create a new figure for the contour plot
    fig = plt.figure()
    plt.contour(X, Y, res_rbf, levels=30, cmap='viridis')

    # Add color bar
    plt.colorbar()

    # Save the contour plot
    plt.savefig('1RBF_DixonPrice_contour.pdf')
    plt.close()  # Close the figure after saving it

"""# 7. RMTS Models

The RMTB and RMTC models are suitable for low-dimensional problems.

## 7.1 RMTB Model
"""

if compiled_available:
    ########### The RMTB model

    t = RMTB(
        xlimits=fun.xlimits,
        min_energy=True,
        nonlinear_maxiter=20,
        print_prediction=False,
    )
    t.set_training_values(xt, yt[:, 0])
    # Add the gradient information
    #    for i in range(ndim):
    #        t.set_training_derivatives(xt,yt[:, 1+i].reshape((yt.shape[0],1)),i)
    t.train()

    # Prediction of the validation points
    y = t.predict_values(xtest)
    print("RMTB,  err: " + str(compute_rms_error(t, xtest, ytest)))
    # plot prediction/true values
    if plot_status:
        fig = plt.figure()
        plt.plot(ytest, ytest, "-", label="$y_{true}$")
        plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

        plt.xlabel("$y_{true}$")
        plt.ylabel(r"$\hat{y}$")

        plt.legend(loc="upper left")
        plt.title("RMTB: validation of the prediction model")


    if plot_status:
        plt.savefig('RMTB: validation of the prediction model.png')

"""## 7.2 RMTC Model"""

if compiled_available:
    ########### The RMTC model

    t = RMTC(
        xlimits=fun.xlimits,
        min_energy=True,
        nonlinear_maxiter=20,
        print_prediction=False,
    )
    t.set_training_values(xt, yt[:, 0])
    #    # Add the gradient information
    #    for i in range(ndim):
    #        t.set_training_derivatives(xt,yt[:, 1+i].reshape((yt.shape[0],1)),i)

    t.train()

    # Prediction of the validation points
    y = t.predict_values(xtest)
    print("RMTC,  err: " + str(compute_rms_error(t, xtest, ytest)))
    # plot prediction/true values
    if plot_status:
        fig = plt.figure()
        plt.plot(ytest, ytest, "-", label="$y_{true}$")
        plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

        plt.xlabel("$y_{true}$")
        plt.ylabel(r"$\hat{y}$")

        plt.legend(loc="upper left")
        plt.title("RMTC model: validation of the prediction model")

    if plot_status:
        plt.savefig('RMTC model: validation of the prediction model.png')

"""# 8. Computation of error metrics

We compute several types of error,  
- the  mean square error
$$
\text{MSE} = \sum\limits_{i=1}^{n_\text{test}}\left(y\left(\textbf{x}_{\text{test},i}\right)-\hat{y}\left(\textbf{x}_{\text{test},i}\right)\right)^2
$$
- the relative error (RE in %)
$$
\text{RE} = \frac{\sqrt{\sum\limits_{i=1}^{n_\text{test}}\left(y\left(\textbf{x}_{\text{test},i}\right)-\hat{y}\left(\textbf{x}_{\text{test},i}\right)\right)^2}}{\sqrt{\sum\limits_{i=1}^{n_{\text{test}}}\left(y\left(\textbf{x}_{\text{test},i}\right)\right)^2}}100,
$$
-  the Coefficient of variation or Coefficient of correlation (R2)
$$
\text{R2} = 1- \frac{{\sum\limits_{i=1}^{n_\text{test}}\left(y\left(\textbf{x}_{\text{test},i}\right)-\hat{y}\left(\textbf{x}_{\text{test},i}\right)\right)^2}}{\sqrt{\sum\limits_{i=1}^{n_{\text{test}}}\left(y\left(\textbf{x}_{\text{test},i}  \right)- \bar y \right)^2}},
$$
where  ${\displaystyle {\bar {y}}}$  is the mean of the observed data:
$${\bar {y}}=\frac {1}{n_{\text{test}}}\sum _{i=1}^{n_{\text{test}}}\hat{y}\left(\textbf{x}_{\text{test},i}\right)$$
An R2 of 1 indicates that the regression line perfectly fits the data.



More metrics computed in scikit-learn are given in http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics
"""

from sklearn.metrics import mean_squared_error, r2_score


# The mean squared error
print("Mean squared error: %.2f" % mean_squared_error(ytest, y))
# Explained variance score: 1 is perfect prediction
print("Variance score: %.2f" % r2_score(ytest, y))

"""# 9. Use of derivatives

Generate the data for the training and the test set
"""

# Computation of the gradient for the DOE points
# Compute the gradient
for i in range(ndim):
    yd = fun(xt, kx=i)
    yt = np.concatenate((yt, yd), axis=1)

ydtest = np.zeros((ntest, ndim))
for i in range(ndim):
    ydtest[:, i] = fun(xtest, kx=i).T

"""# 9.1 GEKPLS model using 2 PLS components and 1 approximating point

Training phase
"""

from smt.surrogate_models import DesignSpace

########### The GEKPLS model using 1 approximating point and 2 components
design_space = DesignSpace(fun.xlimits)
# 'n_comp' must be an integer in [1,ndim[,  'theta0' a list of n_comp values
n_comp = 2
t = GEKPLS(
    n_comp=n_comp,
    theta0=n_comp * [1e-2],
    design_space=design_space,
    delta_x=1e-2,
    extra_points=1,
    print_prediction=False,
)
t.set_training_values(xt, yt[:, 0])
# Add the gradient information
for i in range(ndim):
    t.set_training_derivatives(xt, yt[:, 1 + i].reshape((yt.shape[0], 1)), i)

t.train()

"""Prediction phase for the function"""

# Prediction of the validation points
y = t.predict_values(xtest)
print("GEKPLS1,  err: " + str(compute_rms_error(t, xtest, ytest)))
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("GEKPLS model: validation of the prediction model")

"""Prediction phase for the derivatives"""

# Prediction of the derivatives with regards to each direction space
yd_prediction = np.zeros((ntest, ndim))
for i in range(ndim):
    yd_prediction[:, i] = t.predict_derivatives(xtest, kx=i).T
    print(
        "GEKPLS1, err of the "
        + str(i + 1)
        + "-th derivative: "
        + str(compute_rms_error(t, xtest, ydtest[:, i], kx=i))
    )

    if plot_status:
        plt.plot(ydtest[:, i], ydtest[:, i], "-.")
        plt.plot(ydtest[:, i], yd_prediction[:, i], ".")

    if plot_status:
        plt.savefig('designspace.png')

"""# 9.2 GEKPLS model using 2 PLS components and 2 approximating points

Training phase
"""

# 'n_comp' must be an integer in [2,ndim[,  'theta0' a list of n_comp values
########### The GEKPLS model using 1 approximating point and 2 components
design_space = DesignSpace(fun.xlimits)
# 'n_comp' must be an integer in [1,ndim[,  'theta0' a list of n_comp values
n_comp = 2
t = GEKPLS(
    n_comp=n_comp,
    theta0=n_comp * [1e-2],
    design_space=design_space,
    delta_x=1e-2,
    extra_points=2,
    print_prediction=False,
)

t.set_training_values(xt, yt[:, 0])
# Add the gradient information
for i in range(ndim):
    t.set_training_derivatives(xt, yt[:, 1 + i].reshape((yt.shape[0], 1)), i)

t.train()

"""Prediction for the function"""

# Prediction of the validation points
y = t.predict_values(xtest)
print("GEKPLS2,  err: " + str(compute_rms_error(t, xtest, ytest)))
if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("GEKPLS model: validation of the prediction model")

"""Prediction for the derivatives"""

# Prediction of the derivatives with regards to each direction space
yd_prediction = np.zeros((ntest, ndim))
for i in range(ndim):
    yd_prediction[:, i] = t.predict_derivatives(xtest, kx=i).T
    print(
        "GEKPLS2, err of the "
        + str(i + 1)
        + "-th derivative: "
        + str(compute_rms_error(t, xtest, ydtest[:, i], kx=i))
    )

    if plot_status:
        plt.plot(ydtest[:, i], ydtest[:, i], "-.")
        plt.plot(ydtest[:, i], yd_prediction[:, i], ".")

    if plot_status:
        plt.savefig('GAKLE.png')

"""# 10. Marginal Gaussian Process for high dimensional input space

To use MGP, the input space must be normalized $x \in [-1,1]^d$
"""

n_comp = 1
# To  use MGP the input space must be in [-1, 1]^d, so here we divide by 2
x_train = xt / 2
y_train = yt[:, 0]
x_test = xtest / 2

t = MGP(n_comp=n_comp, theta0=[1e-2] * n_comp * ndim, print_prediction=False)
t.set_training_values(x_train, y_train)
t.train()

# Get the transfer matrix A: u=Ax
emb = t.embedding["C"]
u_train = t.get_u_from_x(x_train)

# Compute the smallest box containing all points of A  (dimension = n_comp)
upper = np.sum(np.abs(emb), axis=0)
lower = -upper

# To have some points to validat on a plot
n_plot = 300
u_plot = np.zeros((n_plot, n_comp))
for ii in range(n_comp):
    u_plot[:, ii] = np.atleast_2d(
        np.linspace(lower[ii], upper[ii], n_plot)
    )  # u_plot is in dimension = n_comp

x_plot = t.get_x_from_u(u_plot)  # Get corresponding points in Omega (dimension = ndim)
y_plot_true = fun(x_plot)
y_plot_pred = t.predict_values(x_plot)
sigma_MGP = t.predict_variances(x_plot)

y = t.predict_values(x_test)

# Prediction of the validation points
print("MGP,  err: " + str(compute_rms_error(t, xtest, ytest)))

# Plots if n_comp=1
if n_comp == 1:
    fig, ax = plt.subplots()
    ax.plot(u_plot, y_plot_pred, label="Predicted")
    ax.plot(u_plot, y_plot_true, "k--", label="True")
    ax.plot(u_train, y_train, "k+", mew=3, ms=10, label="Train")

    ax.set(xlabel="x", ylabel="y", title="MGP")
    fig.legend(loc="upper center", ncol=2)
    fig.tight_layout()
    fig.subplots_adjust(top=0.74)
    plt.savefig('gausian1.png')

    # plot prediction/true values
    fig = plt.figure()
    plt.plot(y_plot_true, y_plot_true, "-", label="$y_{true}-plot$")
    plt.plot(y_plot_true, y_plot_pred, "r.", label=r"$\hat{y}-plot$ MGP")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("Kriging model: validation of the prediction model 2D case")


if plot_status:
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title("MGP model: validation of the prediction model")

if plot_status:
    plt.savefig('gaustian2.png')

"""# 11. Multiple outputs

<div class="alert alert-info fade in" id="d110">
In cas you have multiple outputs $y \in \mathbb{R}^{n_y}$, one possibility is to add an external loop.  The correlations between ouputs are not taken into account.
    </div>
"""

# Compute the ny outputs
yt = fun(xt)
# here y2= 2*y1
ny = 2
yt_ny = np.zeros((np.shape(yt)[0], ny))
for ii in range(ny):
    yt_ny[:, ii] = (ii + 1) * yt[:, 0]


# Construction of the validation points
ytest_ny = np.zeros((np.shape(ytest)[0], ny))
for ii in range(ny):
    ytest_ny[:, ii] = (ii + 1) * ytest[:, 0]

# To build a list of models
list_t = []
########### The Kriging model
for iny in range(ny):
    print("Output ", iny)
    # The variable 'theta0' is a list of length ndim.
    t = KRG(theta0=[1e-2] * ndim, print_prediction=False)
    t.set_training_values(xt, yt_ny[:, iny])

    t.train()
    list_t.append(t)

    # Prediction of the validation points
    y = t.predict_values(xtest)
    print("Kriging,  err: " + str(compute_rms_error(t, xtest, ytest_ny[:, iny])))
    if plot_status:
        # Plot the function, the prediction and the 95% confidence interval based on
        # the MSE
        fig = plt.figure()
        plt.plot(ytest_ny[:, iny], ytest_ny[:, iny], "-", label="$y_{true}$")
        plt.plot(ytest_ny[:, iny], y, "r.", label=r"$\hat{y}$")

        plt.xlabel("$y_{true}$")
        plt.ylabel(r"$\hat{y}$")

        plt.legend(loc="upper left")
        plt.title(
            "Kriging model: validation of the prediction model for output " + str(iny)
        )

    if plot_status:
        plt.savefig('multi-outpu.png')

    # Value of theta
    print("theta values for output ", iny, " = ", t.optimal_theta)

"""<div class="alert alert-info fade in" id="d110">
Another possibility is to consider $y$ with multiple outputs but the correlations are not takin into account (similar as the external loop)
    </div>
"""

# Compute the ny outputs
# here y2= 2*y1
ny = 2
multiyt = np.hstack((yt, 2 * yt))
# Construction of the validation points
multiytest = np.hstack((ytest, 2 * ytest))

# The variable 'theta0' is a list of length ndim.
tmulti = KRG(theta0=[1e-2] * ndim, print_prediction=False)
tmulti.set_training_values(xt, multiyt)

tmulti.train()

# Prediction of the validation points
ymulti = tmulti.predict_values(xtest)

for iny in range(ny):
    print("Kriging,  err: " + str(compute_rms_error(tmulti, xtest, multiytest[:, iny])))
    if plot_status:
        # Plot the function, the prediction and the 95% confidence interval based on
        # the MSE
        fig = plt.figure()
        plt.plot(multiytest[:, iny], multiytest[:, iny], "-", label="$y_{true}$")
        plt.plot(multiytest[:, iny], ymulti[:, iny], "r.", label=r"$\hat{y}$")

        plt.xlabel("$y_{true}$")
        plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title(
        "Kriging model: validation of the prediction model for multiple outputs with output = "
        + str(iny)
    )

    if plot_status:
        plt.show()

"""Between the two methods the error is different!

# 12. Mixture of experts to compare different surrogate models
"""

from smt.applications import MOE

# Only 1 cluster is considered and the surroagte models are compared on a test set from the training database
moe = MOE(n_clusters=1)
moe = MOE(n_clusters=1, xtest=xtest, ytest=ytest)

# to choose some restrictions of the available models use allow option
moe = MOE(n_clusters=1, allow=["KRG", "LS", "QP"])

print("MOE enabled experts: ", moe.enabled_experts)


moe.set_training_values(xt, yt[:, 0])
moe.train()


print("Best model found with MOE", moe._experts[0].name)
if (
    (moe._experts[0].name == "Kriging")
    or (moe._experts[0].name == "KPLS")
    or (moe._experts[0].name == "KPLSK")
):
    print("Correlation parameter of this model:", moe._experts[0].options["corr"])
    print("Regression parameter of this model:", moe._experts[0].options["poly"])


# Prediction of the validation points
y = moe.predict_values(xtest)
print("MOE + 1 cluster,  err: " + str(compute_rms_error(moe, xtest, ytest)))
if plot_status:
    # Plot the function, the prediction and the 95% confidence interval based on
    # the MSE
    fig = plt.figure()
    plt.plot(ytest, ytest, "-", label="$y_{true}$")
    plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

    plt.xlabel("$y_{true}$")
    plt.ylabel(r"$\hat{y}$")

    plt.legend(loc="upper left")
    plt.title(
        "MOE 1 cluster with "
        + str(moe._experts[0].name)
        + ": validation of the prediction model"
    )

if plot_status:
    plt.savefig('MOE.png')


# to add a validation set (xtest,ytest) in order to compare different surrogate models

"""# 13. Saving surrogate models

As written in the documentation (https://smt.readthedocs.io/en/latest/_src_docs/surrogate_models.html), you can save some surrogate models
"""

import pickle

# For saving models written in Python
filename = "moe.pkl"
with open(filename, "wb") as f:
    pickle.dump(moe, f)
# For loading the model
moe_load = None
with open(filename, "rb") as f:
    moe_load = pickle.load(f)

# to use the surrogate model
y_load = moe_load.predict_values(xtest)
print(y[1:10])
print(y_load[1:10])

"""# 14.  Kriging with integer variable"""

from smt.surrogate_models import KRG
from smt.applications.mixed_integer import MixedIntegerKrigingModel
from smt.utils.design_space import DesignSpace, IntegerVariable

# definition of the 1D function
def f(X):
    x = X[:, 0]
    return (x - 3.5) * np.sin((x - 3.5) / (np.pi))


bounds = np.array([[0, 25]])
npt = 26
Xsol = np.linspace(bounds[0][0], bounds[0][1], npt)
Xs = Xsol[:, np.newaxis]
Ysol = f(Xs)

# training the model
xdoe = np.atleast_2d([0, 1, 2, 4, 10, 18, 24]).T
ydoe = f(xdoe)
n_doe = xdoe.size


# to define the model
design_space = DesignSpace(
    [
        IntegerVariable(bounds[0][0], bounds[0][1]),
    ]
)
sm = MixedIntegerKrigingModel(surrogate=KRG(design_space=design_space, theta0=[1e-2]))

sm.set_training_values(xdoe, ydoe)
sm.train()

# predictions
xplot = np.atleast_2d(np.linspace(0, 25.0, 300)).T
yplot = f(np.floor(xplot))
y = sm.predict_values(xplot)  # predictive mean
var = sm.predict_variances(xplot)  # predictive variance

# plotting predictions +- 3 std confidence intervals
plt.rcParams["figure.figsize"] = [8, 4]
plt.fill_between(
    np.ravel(xplot),
    np.ravel(y - 3 * np.sqrt(var)),
    np.ravel(y + 3 * np.sqrt(var)),
    alpha=0.2,
    label="3-sd confidence intervals",
)
plt.scatter(xdoe, ydoe, marker="o", color="blue", s=40, label="Training data set")
plt.scatter(Xsol, Ysol, color="orange", marker="d", s=30, label="Validation data set")
plt.plot(xplot, y, label="mean")
plt.title("Kriging model with integer inputs")
plt.legend(loc=0)
plt.xlabel(r"$x$")
plt.ylabel(r"$y$")
plt.savefig('mixed_integer.png')

"""<div class="alert alert-info fade in" id="d110">
To deal with mixed integer variables or mixed hierarchical variables, a dedicated notebook is available
    
    https://github.com/SMTorg/smt/blob/master/tutorial/SMT_MixedInteger_application.ipynb
    
</div>

# 15.  Kriging with a large database: Sparse GP

When the number of training points is too large,  the idea is to reduce the dimension of the correlation matrix by using inducing points.
    
If $N$ is is number of trianing points, sparse GPs consider a set of inducing points $M$ to approximate the posterior Gaussian distribution with a low-rank representation ($M << N$). Thus, this method enables accurate modeling of large datasets while preserving computational efficiency (typically time and memory for some chosen $M$).

In SMT two methods are used: Fully Independent Training Conditional (FITC) method and the Variational Free Energy (VFE) approximation are implemented inspired from inference methods developed in the GPy project (https://github.com/SheffieldML/GPy).
"""

from smt.surrogate_models import SGP


def f_obj(x):
    import numpy as np

    return (
        np.sin(3 * np.pi * x)
        + 0.3 * np.cos(9 * np.pi * x)
        + 0.5 * np.sin(7 * np.pi * x)
    )


# random generator for reproducibility
rng = np.random.RandomState(0)

# Generate training data
nt = 200
# Variance of the gaussian noise on our trainingg data
eta2 = [0.01]
gaussian_noise = rng.normal(loc=0.0, scale=np.sqrt(eta2), size=(nt, 1))
xt = 2 * rng.rand(nt, 1) - 1
yt = f_obj(xt) + gaussian_noise

"""Here $N=200$  and we have to choose $M$"""

# Pick inducing points randomly in training data
n_inducing = 30
random_idx = rng.permutation(nt)[:n_inducing]
Z = xt[random_idx].copy()

sgp = SGP()
sgp.set_training_values(xt, yt)
sgp.set_inducing_inputs(Z=Z)
# sgp.set_inducing_inputs()  # When Z not specified n_inducing points are picked randomly in traing data
sgp.train()

x = np.linspace(-1, 1, nt + 1).reshape(-1, 1)
y = f_obj(x)
hat_y = sgp.predict_values(x)
var = sgp.predict_variances(x)

# plot prediction
plt.figure(figsize=(14, 6))
plt.plot(x, y, "C1-", label="target function")
plt.scatter(xt, yt, marker="o", s=10, label="observed data")
plt.plot(x, hat_y, "k-", label="Sparse GP")
plt.plot(x, hat_y - 3 * np.sqrt(var), "k--")
plt.plot(x, hat_y + 3 * np.sqrt(var), "k--", label="99% CI")
plt.plot(Z, -2.9 * np.ones_like(Z), "r|", mew=2, label="inducing points")
plt.ylim([-3, 3])
plt.legend(loc=0)
plt.savefig('tutorial_sgp.png')

"""<div class="alert alert-info fade in" id="d110">
To deal with a large database, more information can be find here
    
`https://github.com/SMTorg/smt-sgp-paper`
        
with a dedicated notebook  available
    
   https://github.com/SMTorg/smt-sgp-paper/blob/main/sparse_gp_analytic.ipynb
    
</div>

and a associated paper https://www.mdpi.com/2226-4310/11/4/260

Valayer, H., Bartoli, N., Castaño-Aguirre, M., Lafage, R., Lefebvre, T., López-Lopera, A. F., & Mouton, S. (2024). A Python Toolbox for Data-Driven Aerodynamic Modeling Using Sparse Gaussian Processes. Aerospace, 11(4), 260.

# 16.  Kriging with GP build in Rust (GPX)
"""

# to install the egobox dependency for the GP build in Rust
# !pip install smt[gpx]

from smt.surrogate_models import GPX

xt = np.array([0.0, 1.0, 2.0, 3.0, 4.0])
yt = np.array([0.0, 1.0, 1.5, 0.9, 1.0])

sm = GPX(theta0=[1e-2])
sm.set_training_values(xt, yt)
sm.train()

num = 100
x = np.linspace(0.0, 4.0, num)
y = sm.predict_values(x)
# estimated variance
s2 = sm.predict_variances(x)

_, axs = plt.subplots(1)
# add a plot with variance
axs.plot(xt, yt, "o")
axs.plot(x, y)
axs.fill_between(
    np.ravel(x),
    np.ravel(y - 3 * np.sqrt(s2)),
    np.ravel(y + 3 * np.sqrt(s2)),
    color="lightgrey",
)
axs.set_xlabel("x")
axs.set_ylabel("y")
axs.legend(
    ["Training data", "Prediction", "Confidence Interval 99%"],
    loc="lower right",
)

plt.savefig('tutorial1.png')

# to use KPLS to reduce the input dimension
sm2 = GPX(theta0=[1e-2], kpls_dim=1)
sm2.set_training_values(xt, yt)
sm2.train()

num = 100
x = np.linspace(0.0, 4.0, num)
y = sm2.predict_values(x)
# estimated variance
s2 = sm2.predict_variances(x)

_, axs = plt.subplots(1)
# add a plot with variance
axs.plot(xt, yt, "o")
axs.plot(x, y)
axs.fill_between(
    np.ravel(x),
    np.ravel(y - 3 * np.sqrt(s2)),
    np.ravel(y + 3 * np.sqrt(s2)),
    color="lightgrey",
)
axs.set_xlabel("x")
axs.set_ylabel("y")
axs.legend(
    ["Training data", "Prediction", "Confidence Interval 99%"],
    loc="lower right",
)

plt.savefig('tutorial.png')