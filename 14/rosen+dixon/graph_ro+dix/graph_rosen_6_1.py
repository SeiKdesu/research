# -*- coding: utf-8 -*-
"""GAT_Simplified_Graham,_Jessica_GAE_Clustering_Phase_1_KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/jegraham/1-GNN-Clustering/blob/main/GAT_Simplified_Graham%2C_Jessica_GAE_Clustering_Phase_1_KMeans.ipynb

# Simple K-Means GNN Implementation (Step 1)

This is the initial GNN implementation as referenced in WIDECOMM 2023 Paper Submission. Our implementation uses encoders and decoders with GAT, GCN, and GraphSAGE Layers. Parameters can be modified under the 'Testing Parameters' Section and will be implemented throughout the code.

## Import

### Import Libraries
"""
from datetime import datetime

# 現在の時刻を取得
current_time = datetime.now()
name = f'{current_time}_Spectural'
def visualize_graph(G, color,i):
    plt.figure(figsize=(3, 3))
    plt.xticks([])
    plt.yticks([])

    # ノードの範囲ごとに縦1列に配置するための位置を設定
    pos = {}

    # 各範囲ごとにノードを縦1列に並べる
    ranges = [[0, 1], [2, 3, 4, 5, 6, 7, 8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31], [32]]
    x_offset = 0  # X軸のオフセット

    # ノードを正しく配置するためにループを修正
    for r in ranges:
        for i, node in enumerate(r):
            pos[node] = (x_offset, -i)  # Y座標は負の値に設定
        x_offset += 1  # 次の列に移動

    # エッジの重みに基づいて太さを決定
    weights = nx.get_edge_attributes(G, 'weight')
    default_width = 1.0
    edge_widths = [weights[edge] if edge in weights else default_width for edge in G.edges()]

    # グラフを描画
    plt.figure(figsize=(8, 8))
    nx.draw_networkx_nodes(G, pos, node_size=700, node_color=color, cmap=plt.cm.rainbow)
    nx.draw_networkx_labels(G, pos)
    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='gray', arrows=True)

    # 画像を保存
    if(i==1):
        plt.savefig(f'acc_loss/{name}_rosencrok_teacher_.png')  # 保存

    plt.savefig(f'acc_loss/{name}_rosencrok_predict_.png')  # 保存


import os
import os.path as osp
import shutil
import pandas as pd
import random
import datetime

# libraries for the files in google drive
from pydrive.auth import GoogleAuth
# from google.colab import drive
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials

import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

# GPU Usage Guide - https://medium.com/@natsunoyuki/speeding-up-model-training-with-google-colab-b1ad7c48573e
if torch.cuda.is_available():
    device_name = torch.device("cuda")
else:
    device_name = torch.device('cpu')
print("Using {}.".format(device_name))


from collections import Counter
import matplotlib.pyplot as plt
import math
import networkx as nx
import numpy as np
from scipy.spatial.distance import cdist, squareform
from scipy import stats
from sklearn.cluster import KMeans, MeanShift, AffinityPropagation, FeatureAgglomeration, SpectralClustering, MiniBatchKMeans, Birch, DBSCAN, OPTICS, AgglomerativeClustering
from sklearn.impute import SimpleImputer
from sklearn.mixture import GaussianMixture
from sklearn.metrics import confusion_matrix, pairwise_distances, davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from torch_geometric.data import Data, InMemoryDataset, download_url
from torch_geometric.loader import DataLoader
from torch_geometric.datasets import Planetoid, TUDataset
import torch_geometric.transforms as T
from torch_geometric.nn import GCNConv, SAGEConv, GAE, GINConv, GATConv
from torch_geometric.utils import train_test_split_edges, to_networkx, from_networkx, to_dense_adj
from torch_geometric.transforms import NormalizeFeatures, ToDevice, RandomLinkSplit, RemoveDuplicatedEdges
import torch.nn.functional as F



"""### Import the Dataset

Process the Data Frame - Modified Code from - https://github.com/jegraham/csv_to_dataframe_to_graph/blob/master/.idea/csv_to_datadrame_conversion.py
"""

# from google.colab import files


"""## Testing Parameters"""

# Define the root directory where the dataset will be stored
root = './'
version = 'v1'
run_id = 'GAT_1000_k_50_dist_150_250_500_transform'

# File Path
folder_path = f'./results/{run_id}_{version}/'
os.makedirs(folder_path, exist_ok=True)


# Define the Number of Clusters
num_clusters = 3

K = num_clusters
clusters = []

# num_Infrastructure = 10 #The number of RSU and Towers in the Dataset (always at the start of the dataset)
# max_dist_tower = 500 #V2I
# max_dist_rsu = 250 #V2R
# max_dist = 150 #V2V

# Channel Parameters & GAE MODEL
in_channels = 6
hidden_channels = 20
out_channels = 1

# Transform Parameters
transform_set = True

# Optimizer Parameters (learning rate)
learn_rate = 0.0001

# Epochs or the number of generation/iterations of the training dataset
# epoch and n_init refers to the number of times the clustering algorithm will run different initializations
epochs = 300
n = 1000

"""# Run GNN

## InMemory Dataset

Convert Dataset to same format as Planetoid - https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html
"""

src=[]
dst=[]
for j in range(6):
    for i in range(20):
        src.append(j)

for i in range(6,26):
    src.append(i)
for j in range(6):
    for i in range(6,26):
        dst.append(i)
for i in range(20):
    dst.append(26)

# src=[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31]
# dst=[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32]
edge_index=torch.tensor([src,dst],dtype=torch.long)

print(edge_index.shape)
# input_file_path = 'data.txt'


# # 入力ファイルを読み込み、カンマ区切りに変換
# with open(input_file_path, 'r') as infile:
#     # 各行を取り込み、空白をカンマで区切る
#     lines = infile.readlines()
#     output_numbers = []
    
#     for line in lines:
#         # 行の前後の空白を取り除き、空白を使って数値に変換
#         numbers = [float(num) for num in line.strip().split()]
#         output_numbers.extend(numbers) 
# tmp=output_numbers
tmp=[1.00000000e+00 ,5.19676425e-07, 2.20377808e-06 ,2.02958199e-05,
  8.36487992e-07 ,4.09179058e-06, 2.66318144e-05 ,7.27136548e-13,
  1.09602525e-10, 8.95267142e-07 ,3.29253719e-07, 1.67064438e-05,
  8.18177414e-12 ,3.56605944e-08, 5.44989933e-12, 3.76171053e-06,
  2.93765128e-06, 4.08722063e-09,1.57529558e-09, 1.24448152e-11,
  1.00000000e+00,1.00000000e+00 ,5.19676425e-07, 2.20377808e-06 ,2.02958199e-05,
  8.36487992e-07 ,4.09179058e-06, 2.66318144e-05 ,7.27136548e-13,
  1.09602525e-10, 8.95267142e-07 ,3.29253719e-07, 1.67064438e-05,
  8.18177414e-12 ,3.56605944e-08, 5.44989933e-12, 3.76171053e-06,
  2.93765128e-06, 4.08722063e-09,1.57529558e-09, 1.24448152e-11,
  1.00000000e+00,1.00000000e+00 ,5.19676425e-07, 2.20377808e-06 ,2.02958199e-05,
  8.36487992e-07 ,4.09179058e-06, 2.66318144e-05 ,7.27136548e-13,
  1.09602525e-10, 8.95267142e-07 ,3.29253719e-07, 1.67064438e-05,
  8.18177414e-12 ,3.56605944e-08, 5.44989933e-12, 3.76171053e-06,
  2.93765128e-06, 4.08722063e-09,1.57529558e-09, 1.24448152e-11,
  1.00000000e+00,1.00000000e+00 ,5.19676425e-07, 2.20377808e-06 ,2.02958199e-05,
  8.36487992e-07 ,4.09179058e-06, 2.66318144e-05 ,7.27136548e-13,
  1.09602525e-10, 8.95267142e-07 ,3.29253719e-07, 1.67064438e-05,
  8.18177414e-12 ,3.56605944e-08, 5.44989933e-12, 3.76171053e-06,
  2.93765128e-06, 4.08722063e-09,1.57529558e-09, 1.24448152e-11,
  1.00000000e+00,1.00000000e+00 ,5.19676425e-07, 2.20377808e-06 ,2.02958199e-05,
  8.36487992e-07 ,4.09179058e-06, 2.66318144e-05 ,7.27136548e-13,
  1.09602525e-10, 8.95267142e-07 ,3.29253719e-07, 1.67064438e-05,
  8.18177414e-12 ,3.56605944e-08, 5.44989933e-12, 3.76171053e-06,
  2.93765128e-06, 4.08722063e-09,1.57529558e-09, 1.24448152e-11,
  1.00000000e+00,1.00000000e+00 ,5.19676425e-07, 2.20377808e-06 ,2.02958199e-05,
  8.36487992e-07 ,4.09179058e-06, 2.66318144e-05 ,7.27136548e-13,
  1.09602525e-10, 8.95267142e-07 ,3.29253719e-07, 1.67064438e-05,
  8.18177414e-12 ,3.56605944e-08, 5.44989933e-12, 3.76171053e-06,
  2.93765128e-06, 4.08722063e-09,1.57529558e-09, 1.24448152e-11,
  1.00000000e+00,  1000.15651619 ,
   386.19524365 ,
   953.15160657 ,
  -492.9951205  ,
  -536.00443132 ,
   751.89856035 ,
  -822.78619322 ,
  -660.19959342 ,
   537.68103495 ,
   323.67540273 ,
  -423.32819901 ,
   202.26553251 ,
   879.2368418  ,
  -151.81679793 ,
  -889.34442433 ,
  -811.54631554 ,
  -512.06855665 ,
  -601.51421529 ,
   418.88006262 ,
   448.46304585 ,
  1092.06692505 ,
]
params=torch.tensor(tmp)
print('これのここｎ',params)
edge_attr=params
np.random.seed(1234)


# ファイル名を指定
file_name = 'pop.txt'

# ファイルを読み込み、行ごとにデータを処理
with open(file_name, 'r') as file:
    lines = file.readlines()

# 読み込んだデータをリスト形式に変換
formatted_weight_data = []
for line in lines:
    # 改行を削除し、スペース区切りで数値を分割
    values = line.strip().split()
    # 各数値をfloat型に変換し、リストに格納
    formatted_weight_data.append([float(values[i]) for i in range(6)])
formatted_weight_data.insert(0, [1.81320003, 1,1,1,1,1])
formatted_weight_data.insert(1, [ -0.91577847 , 1,1,1,1,1])
formatted_weight_data.insert(2, [-0.5793548   , 1,1,1,1,1])
formatted_weight_data.insert(3, [1.52467202  , 1,1,1,1,1])
formatted_weight_data.insert(4, [-0.4182809 , 1,1,1,1,1])
formatted_weight_data.insert(5, [ -1.0243715, 1,1,1,1,1])


formatted_weight_data.append([2092.1954181 , 2095.20433097, 2094.02848538 ,2091.80940453, 2093.54564451,
  2095.63220377])#yの値
x=formatted_weight_data

#x=np.zeros_like(a)
x=torch.tensor(x,dtype=torch.float)
#x=torch.tensor([[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[3],[3],[3]],dtype=torch.float)
y_tmp=[]
for i in range(3):
    y_tmp.append(0)
for i in range(3):
    y_tmp.append(1)
for i in range(21):
    y_tmp.append(2)

y = torch.tensor(y_tmp)









"""## Graph AutoEncoder GAE

Graph AutoEncoders GAE &  
Variational Graph Autoencoders VGAE    

[Tutorial 6 paper](https://arxiv.org/pdf/1611.07308.png)  
[Tutorial 6 code](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/autoencoder.py)

### Load the data
"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

dataset=Data(x=x,edge_index=edge_index,edge_attr=edge_attr,y=y,num_classes=3)
Data.train_mask=np.array([1 for i in range(len(y))])



data = dataset
G=to_networkx(dataset, to_undirected=False)
# visualize_graph(G,color=dataset.y,i=1)
# transform = RemoveDuplicatedEdges()
# data = transform(data)


transform = RandomLinkSplit(
    num_val=0.05,
    num_test=0.15,
    is_undirected=False,
    split_labels=True,
    add_negative_train_samples=True)

train_data, val_data, test_data = transform(data)

# Display Graphs
print(f'Number of graphs: {len(dataset)}')
print('dataset',dataset) ## dataset is vector with size 1 because we have one graph

print(f'Number of features: {dataset.num_features}')
print('------------')

# Print information for initialization
print('data', data)
print('train data',train_data)
print('valid data', val_data)
print('test data', test_data)
print('------------')

print(data.is_directed())

"""## Build Graph for Visualization

### Visualize Entire Data
"""


G = to_networkx(data)
G = G.to_directed()

X = data.x[:,[0,1]].cpu().detach().numpy()
pos = dict(zip(range(X[:, 0].size), X))


# Draw the Graph
fig, ax = plt.subplots(figsize=(10, 10))
ax.scatter(X[:,0], X[:,1], s=20, color='grey')
nx.draw_networkx_nodes(G, pos, node_color='black', node_size=20, ax=ax)
nx.draw_networkx_edges(G, pos, edge_color='grey', ax=ax)
ax.set_xlabel('X')
ax.set_ylabel('Y')
plt.savefig(f'{folder_path}{run_id}_{version}-initial-graph', format='eps', dpi=300)


"""### Define the Encoder
Change the Encoder based on the type testing against
"""

class GCNEncoder(torch.nn.Module):
    def __init__(self, in_channels, hidden_size, out_channels):
      super(GCNEncoder, self).__init__()

      # GCN
      # self.conv1 = GCNConv(in_channels, hidden_size, cached=True) # cached only for transductive learning
      # self.conv2 = GCNConv(hidden_size, out_channels, cached=True) # cached only for transductive learning

      # SAGE
      # self.conv1 = SAGEConv(in_channels, hidden_channels, cached=True) # cached only for transductive learning
      # self.conv2 = SAGEConv(hidden_channels, out_channels, cached=True) # cached only for transductive learning

      # GAT
      self.in_head = 8
      self.out_head = 1
      hidden_channels2 = 10
      hidden_channels3=5
      hidden_channels4 =3
      self.conv1 = GATConv(in_channels, hidden_channels, heads=self.in_head, dropout=0.6)
      self.conv2 = GATConv(hidden_channels*self.in_head,hidden_channels2)
      self.conv3 = GATConv(hidden_channels2,hidden_channels3)
      self.conv4 = GATConv(hidden_channels3,hidden_channels4)
      self.conv5 = GATConv(hidden_channels4, out_channels, concat=False)#, heads=self.out_head, dropout=0.6)


    def forward(self, x, edge_index):
      x = self.conv1(x, edge_index).relu()
      x = F.dropout(x, p=0.6, training=self.training)
      x = self.conv2(x,edge_index).relu()
      x = self.conv3(x, edge_index).relu()
      x = self.conv4(x, edge_index).relu()
      x = self.conv5(x,edge_index)
      return x

"""### Define the Autoencoder


"""

# Initialize the Model
model = GAE(GCNEncoder(in_channels, hidden_channels, out_channels))

model = model.to(device)
train_data = train_data.to(device)
test_data = test_data.to(device)
data_ = data.to(device)

# Inizialize the Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)
print(model)

def train(dt):
    model.train()
    optimizer.zero_grad()
    z = model.encode(dt.x, dt.pos_edge_label_index)
    loss = model.recon_loss(z, dt.pos_edge_label_index)
    loss.backward()
    optimizer.step()
    return float(loss)


def test(dt):
    model.eval()
    with torch.no_grad():
      z = model.encode(dt.x, dt.pos_edge_label_index)
  
    return model.test(z, dt.pos_edge_label_index, dt.neg_edge_label_index)

auc_values=[]
ap_values =[]

best_auc = 0.0  # Track the best AUC value
consecutive_epochs = 0  # Track the number of consecutive epochs with AUC not increasing
best_ap = 0.0

import matplotlib.pyplot as plt

# 各エポックのlossとAUCの値を保存するリスト
loss_values = []
auc_values = []
accuracy=[]
best_label=[]
best_loss= 100000000000000
for epoch in range(1, epochs + 1):
    acc=0
    # 訓練データでのlossを取得
    loss = train(train_data)
  
    loss_values.append(loss)

    # テストデータでのAUCとAPを取得
    auc, ap = test(test_data)
    auc_values.append(auc)
    ap_values.append(ap)

    # 各エポックの結果を表示
    # print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))

    # 100エポックごとに表示
    # if (epoch % 100 == 0):
    print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))
    model.eval()
    with torch.no_grad():
        z = model.encode(data_.x, data_.edge_index)
    z = z.cpu().detach().numpy()
    

    
    # gnn_kmeans = KMeans(n_clusters=num_clusters, n_init=n).fit(z)
    # gnn_labels = gnn_kmeans.labels_

    # from sklearn import cluster
    # # SVMの分類器を訓練
    # spkm = cluster.SpectralClustering(n_clusters=num_clusters,affinity="rbf",assign_labels='discretize')
    # res_spkm = spkm.fit(z)
    # gnn_labels = res_spkm.labels_

    # from sklearn.cluster import AffinityPropagation
    # # SVMの分類器を訓練
    # spkm = AffinityPropagation()
    # res_spkm = spkm.fit(z)
    # gnn_labels = res_spkm.labels_

    from sklearn import cluster
    # SVMの分類器を訓練
    spkm = cluster.AgglomerativeClustering(n_clusters=num_clusters,metric='manhattan', linkage='complete')
    res_spkm = spkm.fit(z)
    gnn_labels = res_spkm.labels_

    # from sklearn import cluster
    # # SVMの分類器を訓練
    # spkm = cluster.DBSCAN()
    # res_spkm = spkm.fit(z)
    # gnn_labels = res_spkm.labels_

    if best_loss > loss:
        best_loss = loss
        best_label=gnn_labels
        best_epoch=epoch
    count=0
    for i in range(6): 
        if gnn_labels[i]==dataset.y[i]:
            count += 1
    acc=count/6
    
    accuracy.append(count/6)
    # print(count/2)
    print(gnn_labels)
    if acc > 0.5:
        print('acc 90%',gnn_labels,epoch,acc)
        # break
    # Early stoppingの条件確認
    if (auc >= (best_auc - 0.01 * best_auc)) and (ap >= (best_ap - 0.01 * best_ap)):
        if (auc >= 0.8):
            best_auc = auc
            consecutive_epochs = 0
        if (ap >= 0.5):
            best_ap = ap
            consecutive_epochs = 0
        if (ap >= 0.5) and (auc >= 0.8):
            print("AUC and AP Over GOOD value")
            print(gnn_labels,epoch)
            break
    else:
        consecutive_epochs += 1

    if (consecutive_epochs >= 10):
        print('Early stopping: AUC and AP have not increased by more than 1% for 10 epochs.')
        print(gnn_labels,epoch)
        break
# visualize_graph(G,color=best_label,i=0)
print(epoch,gnn_labels,loss)
print(best_label,best_epoch,best_auc,best_loss)
# 訓練終了後にlossとAUCをプロット
plt.figure()




# Lossのプロット
plt.subplot(2, 1, 1)
plt.plot(range(1, len(loss_values) + 1), loss_values, label='loss')
plt.xlabel('Epoch')
plt.ylabel('LOss')
plt.title('Loss per Epoch')
plt.legend()
plt.savefig(f'acc_loss/{name}_rbf_loss.png')  # 保存
plt.close()

