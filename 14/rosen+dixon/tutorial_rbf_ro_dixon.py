# -*- coding: utf-8 -*-
"""SMT_Tutorial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Qy6YHWJYx2osAdr0Bv3b4XgX2Nr57IT

<a href="https://colab.research.google.com/github/SMTorg/smt/blob/master/tutorial/SMT_Tutorial.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

<div class="jumbotron text-left"><b>
This tutorial describes how to use the SMT toolbox, which is a python toolbox for building a surrogate model.</b></div>

Nathalie BARTOLI ONERA/DTIS/M2CI - May 2024

based on `SMT 2.5.1 version`

<div class="alert alert-info fade in" id="d110">
<p>Latest updates</p>
<ol> - sparse gaussian process for large database </ol>
<ol> - use GPX build in Rust in order to reduce time for trining and prediction (KRG or KPLS models) </ol>
    
 <p>Previous updates</p>  
<ol>     -correct the figures with matplotlib:  ax =  fig.add_subplot(projection='3d') </ol>
<ol> - prediction and variance derivatives for different kernels (squared exponential, absolute exponential, matern 32, matern 52) and trends (constant, linear) </ol>
<ol> -  Kriging with integer variables </ol>   
<ol> -  compare different covariance kernels for the kriging models </ol>
<ol> -  automatic choice of PLS comonents </ol>
<ol> -  example to use Marginal Gaussian Process   </ol>    
<ol> -  example to use multiple outputs (independant outputs) </ol>
<ol> -  example to compare models using Mixture of experts technique  </ol>  
<ol> -  example to save and load models using  `pickle`  </ol>    

</div>

<p class="alert alert-success" style="padding:1em">
To use SMT models, please follow this link : https://github.com/SMTorg/SMT/blob/master/README.md. The documentation is available here: http://smt.readthedocs.io/en/latest/
</p>

The reference paper is available
here https://www.sciencedirect.com/science/article/pii/S0965997818309360?via%3Dihub

or as a preprint: https://www.researchgate.net/profile/Mohamed_Amine_Bouhlel/publication/331976718_A_Python_surrogate_modeling_framework_with_derivatives/links/5cc3cebd299bf12097829631/A-Python-surrogate-modeling-framework-with-derivatives.pdf

Cite us:

M.-A. Bouhlel, J. T. Hwang, N. Bartoli, R. Lafage, J. Morlier, J .R.R.A Martins (2019), A Python surrogate modeling framework with derivatives, Advances in Engineering Software, 102662

P. Saves, R. Lafage, N. Bartoli, Y. Diouane, J. Bussemaker, T. Lefebvre, J. T. Hwang, J. Morlier, J. RRA. Martins (2024). SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes. Advances in Engineering Software, 188, 103571.

@article{SMT2019,
  title={A Python surrogate modeling framework with derivatives},
  author={Mohamed Amine Bouhlel and John T. Hwang and Nathalie Bartoli and R{\'e}mi Lafage and Joseph Morlier and Joaquim R. R. A. Martins},
  journal={Advances in Engineering Software},
  pages={102662},
  year={2019},
  publisher={Elsevier}
}

@article{saves2024smt,
  title={SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes},
  author={Saves, Paul and Lafage, R{\'e}mi and Bartoli, Nathalie and Diouane, Youssef and Bussemaker, Jasper and Lefebvre, Thierry and Hwang, John T and Morlier, Joseph and Martins, Joaquim RRA},
  journal={Advances in Engineering Software},
  volume={188},
  pages={103571},
  year={2024},
  publisher={Elsevier}
}

<div class="alert alert-danger" >
<p> In most of the surrogate models $y\in\mathbb{R}$, so if you have multiple outputs $y\in\mathbb{R}^d$ (which are considered as independant outputs), add an external loop to build one surrogate model per output. The correlations betweens outputs are not taken into account. Section 11 of this notebook presents an example with 2 outputs.
</div>

<div class="alert alert-warning" >
In this notebook, the database is considered to be noise-free. If you want to consider noisy data, please have a look at the dedicated notebook available within SMT.
</div>
"""

from __future__ import print_function, division

# !pip install smt
# # to reduce CPU time
# !pip install numba

"""<div class="alert alert-warning" >
If you use hierarchical variables and the size of your doe greater than 30 points, you may leverage the `numba` JIT compiler to speed up the computation
To do so:
    
 - install numba library
    
     `pip install numba`
    
    
 - and define the environment variable `USE_NUMBA_JIT = 1` (unset or 0 if you do not want to use numba)
    
     - Linux: export USE_NUMBA_JIT = 1
    
     - Windows: set USE_NUMBA_JIT = 1

</div>
"""

# # to check if numba is available
# !pip show numba
# # and then you need to define the environment variable USE_NUMBA_JIT
# !echo "Numba used or not in your environment=" %USE_NUMBA_JIT%

"""# 0. Construction of the DOE points"""

import numpy as np
from smt.utils.misc import compute_rms_error

# from smt.problems import Rosenbrock
from smt.sampling_methods import LHS
from smt.surrogate_models import LS, QP, KPLS, KRG, KPLSK, GEKPLS, MGP

try:
    from smt.surrogate_models import IDW, RBF, RMTC, RMTB

    compiled_available = True
except Exception:
    compiled_available = False

try:
    import matplotlib.pyplot as plt

    plot_status = True
except Exception:
    plot_status = False


import matplotlib.pyplot as plt
from matplotlib import cm


# to ignore warning messages
import warnings

warnings.filterwarnings("ignore")

from datetime import datetime
import os 
# 現在の時刻を取得
current_time = datetime.now()
name = f'{current_time}'
# 保存するディレクトリを作成
if not os.path.exists(name):
    os.makedirs(name)
# ファイルのパスを指定
    file_path = os.path.join(name, "population.txt")




"""## Rosenbrock Function  in dimension N

$$
f(\mathbf{x}) = \sum_{i=1}^{N-1} 100 (x_{i+1} - x_i^2 )^2 + (1-x_i)^2 \quad \mbox{where} \quad \mathbf{x} = [x_1, \ldots, x_N] \in \mathbb{R}^N.
$$

$$x_i \in [-2,2]$$

## Training points and validation points using a LHS algorithm

Here we outline the studied function by plotting the surface representing the function. In addition, we plot also the training points used later to build the surrogate model as well as the validation points which will be used to evaluate the quality of the surrogate model. Both the training point and validation points are generated by a LHS DOE algorithm.

<div class="alert alert-danger" >
<p> In order to have reproducibility of the tests and results, we use  the option random_state to set a seed to the random generator, so that your DOE points are always deterministic. If you don't set a seed, it is different each time.
</div>
"""

########### Initialization of the problem, construction of the training and validation points

ndim = 6
ndoe = 20  # int(10*ndim)
from smt.problems.problem import Problem
class Rosenbrock(Problem):
    def _initialize(self):
        self.options.declare("name", "Rosenbrock", types=str)

    def _setup(self):
        self.xlimits[:, 0] = -2.0
        self.xlimits[:, 1] = 2.0

    def _evaluate(self, x, kx):
        """
        Arguments
        ---------
        x : ndarray[ne, nx]
            Evaluation points.
        kx : int or None
            Index of derivative (0-based) to return values with respect to.
            None means return function value rather than derivative.

        Returns
        -------
        ndarray[ne, 1]
            Functions values if kx=None or derivative values if kx is an int.
        """
        ne, nx = x.shape

        y = np.zeros((ne, 1), complex)
        tmp1 =np.zeros((ne,1),complex)
        tmp2 =np.zeros((ne,1),complex)
        term2 =np.zeros((ne,1),complex)
        if kx is None:
            for ix in range(int(nx/2) - 1):
                tmp1[:,0] += (
                    100.0 * (x[:, ix + 1] - x[:, ix] ** 2) ** 2 + (1 - x[:, ix]) ** 2
                )
          
            # for i in range(int(nx/2) - 1):
                # tmp2[:,0] += (
                #     100.0 * (x[:, ix + 1] - x[:, ix] ** 2) ** 2 + (1 - x[:, ix]) ** 2
                # )
                
            term1 = (x[0] - 1) ** 2
            for i in range(3,6):
                term2[:,0] += sum([i * (2 * x[:,i]**2 - x[:,i-1])**2 ])
            tmp2 = term1 + term2
                # y[:, 0] += (
                #     100.0 * (x[:, ix + 1] - x[:, ix] ** 2) ** 2 + (1 - x[:, ix]) ** 2
                # )
        else:
            print('Wow!')
            if kx < nx - 1:
                y[:, 0] += -400.0 * (x[:, kx + 1] - x[:, kx] ** 2) * x[:, kx] - 2 * (
                    1 - x[:, kx]
                )
            if kx > 0:
                y[:, 0] += 200.0 * (x[:, kx] - x[:, kx - 1] ** 2)
        y = tmp1 + tmp2
        return y
# Define the function
fun = Rosenbrock(ndim=ndim)

# Construction of the DOE
# in order to have the always same LHS points, random_state=1
sampling = LHS(xlimits=fun.xlimits, criterion="ese", random_state=1)
xt = sampling(ndoe)
# Compute the outputs
yt = fun(xt)

# Construction of the validation points
ntest = 200  # 500
sampling = LHS(xlimits=fun.xlimits, criterion="ese", random_state=1)
xtest = sampling(ntest)
ytest = fun(xtest)

# To visualize the DOE points
fig = plt.figure(figsize=(10, 10))
plt.scatter(xt[:, 0], xt[:, 1], marker="x", c="b", s=200, label="Training points")
plt.scatter(
    xtest[:, 0], xtest[:, 1], marker=".", c="k", s=200, label="Validation points"
)
plt.title("DOE")
plt.xlabel("x1")
plt.ylabel("x2")
plt.legend()
plt.savefig('DOE.png')

# # To plot the Rosenbrock function
# x = np.linspace(-2, 2, 50)
# res = []
# for x0 in x:
#     for x1 in x:
#         res.append(fun(np.array([[x0, x1]])))
# res = np.array(res)
# res = res.reshape((50, 50)).T
# X, Y = np.meshgrid(x, x)
# fig = plt.figure(figsize=(15, 10))
# ax = fig.add_subplot(projection="3d")
# surf = ax.plot_surface(
#     X, Y, res, cmap=cm.viridis, linewidth=0, antialiased=False, alpha=0.5
# )

# ax.scatter(
#     xt[:, 0], xt[:, 1], yt, zdir="z", marker="x", c="b", s=200, label="Training point"
# )
# ax.scatter(
#     xtest[:, 0],
#     xtest[:, 1],
#     ytest,
#     zdir="z",
#     marker=".",
#     c="k",
#     s=200,
#     label="Validation point",
# )

# plt.title("Rosenbrock function")
# plt.xlabel("x1")
# plt.ylabel("x2")
# plt.legend()
# plt.savefig('Rosenbrock function.png')

"""Different models will be used and compared:

- Linear Model (first order polynomial)
- Quadratic Model (second order polynomial)
- Kriging Model (also known as Gaussian Process)
- KPLS Model and KPLSK Model (useful for Kriging in high dimension)
- IDW Model (Inverse Distance Weighting)
- RBF Model (Radial Basis Function)
- RMTS Models: RMTB and RMTC (useful for low dimensional problem)

Some metrics to assess some errors are proposed at the end.

Mixture of experts technique will be used to compare different surrogate models.

"""

"""# 6. RBF Model
Here we implement the RBF model.
"""

if compiled_available:
    ########### The RBF model

    t = RBF(print_prediction=False, poly_degree=0)
    t.set_training_values(xt, yt[:, 0])

    t.train()
    with open(file_path, "a") as file:
        file.write(f"{xt},{yt},{t.sol},{t.mtx}\n")
    # Prediction of the validation points
    y = t.predict_values(xtest)
    print("RBF,  err: " + str(compute_rms_error(t, xtest, ytest)))
    # Plot prediction/true values
    if plot_status:
        fig = plt.figure()
        plt.plot(ytest, ytest, "-", label="$y_{true}$")
        plt.plot(ytest, y, "r.", label=r"$\hat{y}$")

        plt.xlabel("$y_{true}$")
        plt.ylabel(r"$\hat{y}$")

        plt.legend(loc="upper left")
        plt.title("RBF model: validation of the prediction model")
        plt.savefig('RBF model: validation of the prediction model.png')

