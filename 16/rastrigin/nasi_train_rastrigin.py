# -*- coding: utf-8 -*-
"""GAT_Simplified_Graham,_Jessica_GAE_Clustering_Phase_1_KMeans.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/jegraham/1-GNN-Clustering/blob/main/GAT_Simplified_Graham%2C_Jessica_GAE_Clustering_Phase_1_KMeans.ipynb

# Simple K-Means GNN Implementation (Step 1)

This is the initial GNN implementation as referenced in WIDECOMM 2023 Paper Submission. Our implementation uses encoders and decoders with GAT, GCN, and GraphSAGE Layers. Parameters can be modified under the 'Testing Parameters' Section and will be implemented throughout the code.

## Import

### Import Libraries
"""
from datetime import datetime

# 現在の時刻を取得
current_time = datetime.now()
name = f'{current_time}_Spectural'
def visualize_graph(G, color, i, file_dir_name):
    plt.figure(figsize=(3, 3))
    plt.xticks([])
    plt.yticks([])

    # ノードの範囲ごとに縦1列に配置するための位置を設定
    pos = {}

    # 各範囲ごとにノードを縦1列に並べる
    ranges = [[0, 1, 2, 3, 4, 5], list(range(6, 108)), [108]]
    x_offset = 0  # X軸のオフセット

    # ノードを正しく配置するためにループを修正
    for r in ranges:
        if r == ranges[0]:  # 最初の範囲に間隔を追加
            y_offset = -10  # 間隔を広げるために大きな負の値を設定
        else:
            y_offset = -1  # 通常の間隔

        for i, node in enumerate(r):
            pos[node] = (x_offset, y_offset * i)  # Y座標は間隔に基づいて設定
        x_offset += 10  # 次の列に移動

    # エッジの重みに基づいて太さを決定
    weights = nx.get_edge_attributes(G, 'weight')
    default_width = 1.0
    edge_widths = [weights[edge] if edge in weights else default_width for edge in G.edges()]

    # グラフを描画
    plt.figure(figsize=(8, 8))
    nx.draw_networkx_nodes(G, pos, node_size=700, node_color=color, cmap=plt.cm.rainbow)
    nx.draw_networkx_labels(G, pos)
    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='gray', arrows=True)

    # 画像を保存
    if i == 1:
        plt.savefig(f'{file_dir_name}/teacher_.png')  # 保存

    plt.savefig(f'{file_dir_name}/predict_.png')  # 保存



import os
import os.path as osp
import shutil
import pandas as pd
import random
import datetime

# libraries for the files in google drive
# from pydrive.auth import GoogleAuth
# from google.colab import drive
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials

import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

# GPU Usage Guide - https://medium.com/@natsunoyuki/speeding-up-model-training-with-google-colab-b1ad7c48573e
if torch.cuda.is_available():
    device_name = torch.device("cuda")
else:
    device_name = torch.device('cpu')
print("Using {}.".format(device_name))


from collections import Counter
import matplotlib.pyplot as plt
import math
import networkx as nx
import numpy as np
from scipy.spatial.distance import cdist, squareform
from scipy import stats
from sklearn.cluster import KMeans, MeanShift, AffinityPropagation, FeatureAgglomeration, SpectralClustering, MiniBatchKMeans, Birch, DBSCAN, OPTICS, AgglomerativeClustering
from sklearn.impute import SimpleImputer
from sklearn.mixture import GaussianMixture
from sklearn.metrics import confusion_matrix, pairwise_distances, davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.datasets import Planetoid, TUDataset
import torch_geometric.transforms as T
from torch_geometric.nn import GCNConv, SAGEConv, GAE, GINConv, GATConv
from torch_geometric.utils import train_test_split_edges, to_networkx, from_networkx, to_dense_adj
from torch_geometric.transforms import NormalizeFeatures, ToDevice, RandomLinkSplit, RemoveDuplicatedEdges
import torch.nn.functional as F
from sklearn import cluster
# 学習率スケジューラー
from torch.optim import lr_scheduler
from torch import nn

"""### Import the Dataset

Process the Data Frame - Modified Code from - https://github.com/jegraham/csv_to_dataframe_to_graph/blob/master/.idea/csv_to_datadrame_conversion.py
"""

# from google.colab import files


"""## Testing Parameters"""

# Define the root directory where the dataset will be stored
root = './'
version = 'v1'
run_id = 'GAT_1000_k_50_dist_150_250_500_transform'

# File Path
folder_path = f'./results/{run_id}_{version}/'
os.makedirs(folder_path, exist_ok=True)


# Define the Number of Clusters
num_clusters = 7

K = num_clusters
clusters = []

# num_Infrastructure = 10 #The number of RSU and Towers in the Dataset (always at the start of the dataset)
# max_dist_tower = 500 #V2I
# max_dist_rsu = 250 #V2R
# max_dist = 150 #V2V

# Channel Parameters & GAE MODEL
in_channels = 6
hidden_channels = 20
out_channels = 3

# Transform Parameters
transform_set = True



# Epochs or the number of generation/iterations of the training dataset
# epoch and n_init refers to the number of times the clustering algorithm will run different initializations
epochs = 100
n = 1000
count_0 = [0]*6
count_1 = [0]*6
count_2 = [0]*6



class GCNEncoder(torch.nn.Module):
    def __init__(self, in_channels, hidden_size, out_channels):
      super(GCNEncoder, self).__init__()

      # GCN
      # self.conv1 = GCNConv(in_channels, hidden_size, cached=True) # cached only for transductive learning
      # self.conv2 = GCNConv(hidden_size, out_channels, cached=True) # cached only for transductive learning

      # SAGE
      # self.conv1 = SAGEConv(in_channels, hidden_channels, cached=True) # cached only for transductive learning
      # self.conv2 = SAGEConv(hidden_channels, out_channels, cached=True) # cached only for transductive learning

      # GAT
      self.in_head = 8
      self.out_head = 1
      hidden_channels2 = 100
      hidden_channels3=80
      hidden_channels4 =40
      hidden_channels5= 20
      self.conv1 = GATConv(in_channels, hidden_channels, heads=self.in_head, dropout=0.6)
      self.conv2 = GATConv(hidden_channels*self.in_head,hidden_channels2)
      self.conv3 = GATConv(hidden_channels2,hidden_channels3)
      self.conv4 = GATConv(hidden_channels3,hidden_channels4)
      self.conv5 = GATConv(hidden_channels4, hidden_channels5)#, heads=self.out_head, dropout=0.6)
      self.conv6 = GATConv(hidden_channels5, out_channels, concat=False)
      

    def forward(self, x, edge_index):
      x = self.conv1(x, edge_index).relu()
      x = F.dropout(x, p=0.6, training=self.training)
      x = self.conv2(x,edge_index).relu()
      x = self.conv3(x, edge_index).relu()
      x = self.conv4(x, edge_index).relu()
      x = self.conv5(x,edge_index).relu()
      x = self.conv6(x,edge_index)
      return x
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
def keep_indices_as_nonzero(arr, indices):
    arr = arr
    indices = np.asarray(indices, dtype=int)
    # 元の配列と同じ形状でゼロ配列を作成
    result = np.zeros_like(arr)
    # 指定されたインデックスに対応する要素をコピー
    result[:,indices] = arr[:,indices]
    return result
def train(dt):
    model.train()
    optimizer.zero_grad()
    z = model.encode(dt.x, dt.pos_edge_label_index)
    with torch.no_grad():
        z = model.encode(data_.x, data_.edge_index)
    z = z.cpu().detach().numpy()



    # spkm = KMeans(n_clusters=num_clusters, n_init=n)
    # gnn_labels = gnn_kmeans.labels_

    # from sklearn import cluster
    # # SVMの分類器を訓練
    # spkm = cluster.SpectralClustering(n_clusters=num_clusters,affinity="rbf",assign_labels='discretize')
    # res_spkm = spkm.fit(z)
    # gnn_labels = res_spkm.labels_

    # from sklearn.cluster import AffinityPropagation
    # # SVMの分類器を訓練
    # spkm = AffinityPropagation()
    # res_spkm = spkm.fit(z)
    # gnn_labels = res_spkm.labels_


    # SVMの分類器を訓練
    spkm = cluster.AgglomerativeClustering(n_clusters=num_clusters, metric='manhattan', linkage='complete')

    res_spkm = spkm.fit(z)
    gnn_labels = res_spkm.labels_
    print(gnn_labels)
    pp,ff = QOL()

    ff_out =objective_function(pp)
    
    # ff_out = ff_out[0]
    pp= pp

    # ff_real = objective_function(pp,dim)
    pp = np.array(pp)
    classfication = gnn_labels
    # classfication = [0,0,2,1,1,2]
    mal_list0=[]
    list0_count=0
    mal_list1=[]
    mal_list2 =[]
    mal_list3=[]
    mal_list4=[]
    mal_list5=[]
    list1_count =0
    # mal_list2=[]
    # list2_count =dim
    
    for count in range(dim):
        if classfication[count] == 0:
            mal_list0.append(count)
            list0_count += 1
        if classfication[count] == 1:
            mal_list1.append(count)
        if classfication[count] == 2:
            mal_list2.append(count)
            list0_count += 1
        if classfication[count] == 3:
            mal_list3.append(count)
        if classfication[count] == 4:
            mal_list4.append(count)
            list0_count += 1
        if classfication[count] == 5:
            mal_list5.append(count)
        

    mal_list0= np.array(mal_list0)
    mal_list1 = np.array(mal_list1)
    mal_list2= np.array(mal_list2)
    mal_list3 = np.array(mal_list3)
    mal_list4= np.array(mal_list4)
    mal_list5 = np.array(mal_list5)

    


    prediction=[]
    prediction1=[]
    prediction2 = []
    
    
    # if list0_count != dim : 
    #     prediction = t_preditct(input)
    # else:
    #     prediction[0] = 0
    # input2 = pp*mal_list1
    # if list1_count != dim:
    #     prediction1 = t_preditct(input2)
    # else:
    #     prediction1.append(0.0)
    # input3 = pp*mal_list2
    # if list2_count == dim: 
    #     prediction2.append(0.0)
    # else:
    #     prediction2 = t_preditct(input3)
    # print(mal_list0)
    # print(mal_list1)
    # print(mal_list2)
    # print(mal_list3)
    # print(mal_list4)
    # print(mal_list5)
    pp1 = keep_indices_as_nonzero(pp, mal_list0)
    pp2 = keep_indices_as_nonzero(pp, mal_list1)
    pp3 = keep_indices_as_nonzero(pp, mal_list2)
    pp4 = keep_indices_as_nonzero(pp, mal_list3)
    pp5 = keep_indices_as_nonzero(pp, mal_list4)
    pp6 = keep_indices_as_nonzero(pp, mal_list5)
    all_prediction1 = objective_function1(pp1)
    all_prediction2 = objective_function1(pp2)
    all_prediction3 = objective_function1(pp3)
    all_prediction4 = objective_function1(pp4)
    all_prediction5 = objective_function1(pp5)
    all_prediction6 = objective_function1(pp6)

    # print("1",pp1)
    # print("2",pp2)
    # print("3",pp3)
    # print("4",pp4)
    # print("5",pp5)
    # print("6",pp6)
  
    all_prediction = all_prediction1 + all_prediction2 + all_prediction3+all_prediction4+all_prediction5+all_prediction6
    ff_out = np.array(ff_out,dtype = np.float32)
    # ff_out = ff_out.reshape(20,1)

    


    # loss_mse= mean_squared_error(ff, all_prediction)
    ff_out = torch.tensor(ff_out,requires_grad=True,dtype=float)
    all_prediction = torch.tensor(all_prediction)
    loss_func=nn.L1Loss()

    loss = loss_func(ff_out,all_prediction)
    print(loss)
    # loss = model.recon_loss(z, dt.pos_edge_label_index)
    loss.backward()
    optimizer.step()
    return float(loss),gnn_labels,z


def test(dt):
    model.eval()
    with torch.no_grad():
      z = model.encode(dt.x, dt.pos_edge_label_index)
  
    return model.test(z, dt.pos_edge_label_index, dt.neg_edge_label_index)
"""# Run GNN

## InMemory Dataset

Convert Dataset to same format as Planetoid - https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html
"""
# from tutorial_rbf import *
from rbf_surrogate_100_train_rastrigin import *
for com in range(1):
    
    src=[]
    dst=[]
    for j in range(6):
        for i in range(101):
            src.append(j)

    for i in range(6,107):
        src.append(i)

    for j in range(6):
        for i in range(6,107):
            dst.append(i)
    for i in range(101):
        dst.append(108)
    edge_index=torch.tensor([src,dst],dtype=torch.long)


###############################################################################
    tmp =[]
    length_matrix = matrix()
    one = length_matrix[0]


    for i in range(dim):
        tmp.append(one)
        
    weight1 = weight()
    weight1=weight1.squeeze(1)
    weight1 = weight1.tolist()

    tmp.append(weight1)


    tmp  = [item for sublist in tmp for item in sublist]
    params=[]
    for item in tmp:
        params.append(item)
    params = torch.tensor(params)
    edge_attr=params

#########################################################################

    # ファイル名を指定
    file_name = make_file_path()

    # ファイルを読み込み、行ごとにデータを処理
    with open(file_name, 'r') as file:
        lines = file.readlines()

    # 読み込んだデータをリスト形式に変換
    num = get_xt()
    num=num[com]
    num.reshape(6)
    formatted_weight_data = []
    formatted_weight_data = xt_all()

    # for i in range(dim-1,-1,-1):
    #     new_array = np.array([num[i],1,1,1,1,1])
    #     formatted_weight_data = np.vstack([new_array,formatted_weight_data])
    new_array = np.array([num[0],0,0,0,0,0])
    formatted_weight_data = np.vstack([new_array,formatted_weight_data])
    new_array = np.array([0,num[1],0,0,0,0])
    formatted_weight_data = np.vstack([new_array,formatted_weight_data])
    new_array = np.array([0,0,num[2],0,0,0])
    formatted_weight_data = np.vstack([new_array,formatted_weight_data])
    new_array = np.array([0,0,0,num[3],0,0])
    formatted_weight_data = np.vstack([new_array,formatted_weight_data])
    new_array = np.array([0,0,0,0,num[4],0])
    formatted_weight_data = np.vstack([new_array,formatted_weight_data])
    new_array = np.array([0,0,0,0,0,num[5]])
    formatted_weight_data = np.vstack([new_array,formatted_weight_data])
        # formatted_weight_data.insert(i, [num[i] , 1,1,1,1,1])
    # formatted_weight_data.insert(1, [-2.20  , 1,1,1,1,1])
    # formatted_weight_data.insert(2, [0.21596364  , 1,1,1,1,1])
    # formatted_weight_data.insert(3, [ 0.70831308    , 1,1,1,1,1])
    # formatted_weight_data.insert(4, [ -3.63318154  , 1,1,1,1,1])
    # formatted_weight_data.insert(5, [-0.62985134, 1,1,1,1,1])
    to_match = np.array([1,1,1,1,1,1])
    formatted_weight_data = np.vstack([formatted_weight_data,to_match])


    num1 = get_yt()
    new_temp= np.array([num1[com][0],1,1,1,1,1])
    formatted_weight_data = np.vstack([formatted_weight_data,new_temp])
    # formatted_weight_data.append([num1[0], 1,1,1,1,1])#yの値
    x=formatted_weight_data

    #x=np.zeros_like(a)
    x=torch.tensor(x,dtype=torch.float)
    #x=torch.tensor([[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[2],[3],[3],[3]],dtype=torch.float)
    y_tmp=[]
    for i in range(6):
        y_tmp.append(i)
    # for i in range(3):
    #     y_tmp.append(1)
    for i in range(102):
        y_tmp.append(6)

    y = torch.tensor(y_tmp)


    """## Graph AutoEncoder GAE

    Graph AutoEncoders GAE &  
    Variational Graph Autoencoders VGAE    

    [Tutorial 6 paper](https://arxiv.org/pdf/1611.07308.png)  
    [Tutorial 6 code](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/autoencoder.py)

    ### Load the data
    """



    dataset=Data(x=x,edge_index=edge_index,edge_attr=edge_attr,y=y,num_classes=7)
    Data.train_mask=np.array([1 for i in range(6)])



    data = dataset
    G=to_networkx(dataset, to_undirected=False)
    dir_file = dirs()
    # visualize_graph(G,color=dataset.y,i=1,file_dir_name=dir_file)
    # transform = RemoveDuplicatedEdges()
    # data = transform(data)

    transform = RandomLinkSplit(
        num_val=0.05,
        num_test=0.15,
        is_undirected=False,
        split_labels=True,
        add_negative_train_samples=True)

    train_data, val_data, test_data = transform(data)

    # Display Graphs
    print(f'Number of graphs: {len(dataset)}')
    print('dataset',dataset) ## dataset is vector with size 1 because we have one graph

    print(f'Number of features: {dataset.num_features}')
    print('------------')

    # Print information for initialization
    print('data', data)
    print('train data',train_data)
    print('valid data', val_data)
    print('test data', test_data)
    print('------------')

    print(data.is_directed())

    """## Build Graph for Visualization

    ### Visualize Entire Data
    """


    G = to_networkx(data)
    G = G.to_directed()

    X = data.x[:,[0,1]].cpu().detach().numpy()
    pos = dict(zip(range(X[:, 0].size), X))


    # Draw the Graph
    fig, ax = plt.subplots(figsize=(10, 10))
    ax.scatter(X[:,0], X[:,1], s=20, color='grey')
    nx.draw_networkx_nodes(G, pos, node_color='black', node_size=20, ax=ax)
    nx.draw_networkx_edges(G, pos, edge_color='grey', ax=ax)
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    plt.savefig(f'{folder_path}{run_id}_{version}-initial-graph', format='eps', dpi=300)


    """### Define the Encoder
    Change the Encoder based on the type testing against
    """

    """### Define the Autoencoder


    """

    # Initialize the Model
    model = GAE(GCNEncoder(in_channels, hidden_channels, out_channels))

    model = model.to(device)
    train_data = train_data.to(device)
    test_data = test_data.to(device)
    data_ = data.to(device)
    # Optimizer Parameters (learning rate)
    learn_rate = 0.00000000001

    # Inizialize the Optimizer
    optimizer = torch.optim.Adam(model.parameters(), lr = learn_rate)
    # 5. CosineAnnealingLR
    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    print(model)



    auc_values=[]
    ap_values =[]

    best_auc = 0.0  # Track the best AUC value
    consecutive_epochs = 0  # Track the number of consecutive epochs with AUC not increasing
    best_ap = 0.0

    import matplotlib.pyplot as plt

    # 各エポックのlossとAUCの値を保存するリスト
    loss_values = []
    auc_values = []
    accuracy=[]
    best_label=[]
    best_loss= 100000000000000
    best_z = np.ones_like((108,2))
    for epoch in range(1, epochs + 1):
        acc=0
        acc_num = 0
        # 訓練データでのlossを取得
        loss,gnn_labels,best_z = train(train_data)
        # loss = int(loss)
        # if loss == 0:
        #     break
        loss_values.append(loss)
        
        # テストデータでのAUCとAPを取得
        auc, ap = test(test_data)
        auc_values.append(auc)
        ap_values.append(ap)

        # 各エポックの結果を表示
        # print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))

        # 100エポックごとに表示
        # if (epoch % 100 == 0):
        # print('Epoch: {:03d}, Loss: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))
        # 学習率の更新    
        scheduler.step()
        model.eval()


        # from sklearn import cluster
        # # SVMの分類器を訓練
        # spkm = cluster.DBSCAN()
        # res_spkm = spkm.fit(z)
        # gnn_labels = res_spkm.labels_

        if best_loss > loss:
            best_loss = loss
            best_label=gnn_labels
            best_epoch=epoch
            visualize_graph(G,color=best_label,i=1,file_dir_name=dir_file)
        count=0
        for i in range(6): 
            if gnn_labels[i]==dataset.y[i]:
                count += 1
        acc=count/6
        
        
        # accuracy.append(count/6)
        # # print(count/2)
        # print(gnn_labels)
        # if acc > 0.5:
        #     print('acc 90%',gnn_labels,epoch,acc)
            # break
        # Early stoppingの条件確認
        # if (auc >= (best_auc - 0.01 * best_auc)) and (ap >= (best_ap - 0.01 * best_ap)):
        #     if (auc >= 0.8):
        #         best_auc = auc
        #         consecutive_epochs = 0
        #     if (ap >= 0.5):
        #         best_ap = ap
        #         consecutive_epochs = 0
        #     if (ap >= 0.5) and (auc >= 0.8):
        #         print("AUC and AP Over GOOD value")
        #         print(gnn_labels,epoch)
        #         break
        # else:
        #     consecutive_epochs += 1

    #     if (consecutive_epochs >= 10):
    #         print('Early stopping: AUC and AP have not increased by more than 1% for 10 epochs.')
    #         print(gnn_labels,epoch)
    #         break
    # # visualize_graph(G,color=best_label,i=0)

        if acc > 0.8:
            print(epoch,gnn_labels,loss)
            print(best_label,best_epoch,best_auc,best_loss)
    # print(count_0,count_1,count_2)
    # visualize_graph(G,color=best_label,i=0,file_dir_name=dir_file)
    # 訓練終了後にlossとAUCをプロット
    print(best_label,best_epoch,best_auc,best_loss)
    plt.figure()




    # Lossのプロット
    plt.subplot(2, 1, 1)
    plt.plot(range(1, len(loss_values) + 1), loss_values, label='loss')
    plt.xlabel('Epoch')
    plt.ylabel('LOss')
    plt.title('Loss per Epoch')
    plt.legend()
    plt.savefig(f'acc_loss/{name}_rbf_loss.png')  # 保存
    plt.close()

plt.close()
# ヒストグラムの作成
# 要素のインデックス
indices = np.arange(len(count_0))  # 0, 1, 2, ... の配列を作成
file_path_count = os.path.join(dir_file,"count.txt")
with open(file_path_count, "a") as file:
    file.write(f"{count_0},{count_1},{count_2}\n")
# 棒グラフの幅
width = 0.25  

# 棒グラフの作成
plt.bar(indices, count_0, width, label='Data 1', color='blue')
plt.bar(indices + width, count_1, width, label='Data 2', color='orange')
plt.bar(indices + 2 * width, count_2, width, label='Data 3', color='green')

# グラフの設定
plt.xlabel('Index')
plt.ylabel('Values')
plt.title('Comparison of Data')
plt.xticks(indices + width, range(len(count_0)))  # 横軸の目盛りを設定
#plt.legend(loc='upper right')  # 凡例の表示

# グラフを表示
plt.tight_layout()  # レイアウトの調整
# グラフを表示
plt.savefig(f'{dir_file}/hist.png')



best_label = np.array(best_label)
# 3Dプロットの準備
fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

# カラーマッピング
colors = {0: 'red', 1: 'blue', 2: 'green',3: 'yellow', 4: 'pink', 5:'purple',6: 'black'}

# 散布図のプロット
for label, color in colors.items():
    subset = best_z[best_label == label]
    indices = np.where(best_label == label)[0]
    ax.scatter(subset[:, 0], subset[:, 1], subset[:, 2],label=f'Label {label}', color=color, alpha=0.6)
    # 各点のインデックスを表示
    for i, (x, y, z) in zip(indices, subset):
        ax.text(x, y, z, str(i), fontsize=30, color=color)

# 軸ラベルとタイトル
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
ax.set_zlabel('Z-axis')
ax.set_title('3D Scatter Plot with Indices and Labels')
ax.legend()
plt.savefig(f'{dir_file}/潜在変数空間.png')
plt.close()



















# 洗剤変数空間の可視化
gnn_kmeans= best_label
gnn_eval_data = data_
gnn_X = gnn_eval_data.x[:,[0,1]].cpu().detach().numpy()
gnn_df = pd.DataFrame(gnn_X, columns = ['X','Y'])

# Adding cluster labels to the DataFrame
gnn_df_with_cluster = gnn_df.copy(deep=True)
gnn_df_with_cluster['cluster'] = best_label

gnn_G = to_networkx(gnn_eval_data)
gnn_G = gnn_G.to_undirected()
gnn_labels = best_label
# gnn_cluster_centers = gnn_kmeans.cluster_centers_


gnn_pos = dict(zip(range(gnn_X[:, 0].size), gnn_X))


fig, ax = plt.subplots(figsize=(10, 10))
ax.scatter(gnn_df_with_cluster['X'], gnn_df_with_cluster['Y'], s=20, color='grey')
# ノードをクラスタごとに色分けして描画
cmap = plt.get_cmap('tab20')

nx.draw_networkx_nodes(gnn_G, gnn_pos, cmap=cmap, node_color = gnn_labels, node_size=1000, ax=ax)
nx.draw_networkx_edges(gnn_G, gnn_pos, edge_color='grey', ax=ax)
# ノード番号（ラベル）を追加
# ラベルを少しずつずらすためのオフセット
offset_x = 0.1
offset_y = 0.0  # Y方向に少しずらす
# ノードの位置を変更してラベルをずらす
gnn_pos_adjusted = {node: (x + offset_x, y + offset_y * node) for node, (x, y) in gnn_pos.items()}
# ノードラベルをずらして描画
nx.draw_networkx_labels(gnn_G, gnn_pos_adjusted, font_size=30, font_color='black', ax=ax)

# nx.draw_networkx_labels(gnn_G, gnn_pos, font_size=8, font_color='black', ax=ax)
print(gnn_G)
print(gnn_pos)
ax.set_xlabel('X')
ax.set_ylabel('Y')
# x軸とy軸に目盛りを表示
ax.tick_params(axis='both', which='major', labelsize=10)
ax.grid(True)  # グリッドを有効化（必要に応じて）
# # x軸とy軸の範囲を設定し、目盛りを自動的に表示
# ax.set_xlim(gnn_df_with_cluster['X'].min() - 10.1, gnn_df_with_cluster['X'].max() + 10.1)
# ax.set_ylim(gnn_df_with_cluster['Y'].min() - 10.1, gnn_df_with_cluster['Y'].max() + 10.1)

# # 自動的に目盛りを有効にする
# ax.xaxis.set_major_locator(plt.AutoLocator())
# ax.yaxis.set_major_locator(plt.AutoLocator())
# 凡例を表示（クラスタの番号と色を示す）
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(i / float(max(gnn_labels) + 1)), markersize=10, label=f'Cluster {i}') for i in range(max(gnn_labels) + 1)]
ax.legend(handles=legend_elements, loc='upper right', title="Clusters")
plt.savefig(f'{folder_path}{run_id}_{version}-kmeans-cluster-node-features-gnn', format='eps', dpi=300)
plt.savefig(f'{dir_file}/潜在変数.png')
plt.close()
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.cluster import KMeans
import pandas as pd

# 使用するノードのインデックスを指定（0~5番目のノードを使用）
selected_nodes = list(range(6))  # 0~5のノードを選択

# 使用するノードの位置とラベルを選択
gnn_pos_selected = {k: gnn_pos[k] for k in selected_nodes}  # ノードの位置を選択したノードのみに絞り込む
gnn_labels_selected = gnn_labels[selected_nodes]  # クラスタラベルを選択したノードのみに絞り込む

# 使用するノードを含むサブグラフを作成
gnn_G_selected = gnn_G.subgraph(selected_nodes)

# Draw the Graph
fig, ax = plt.subplots(figsize=(10, 10))

# ノードをクラスタごとに色分けして描画
cmap = plt.get_cmap('tab20')
nx.draw_networkx_nodes(gnn_G_selected, gnn_pos_selected, cmap=cmap, 
                       node_color=gnn_labels_selected, node_size=1000, ax=ax)

# エッジを描画
nx.draw_networkx_edges(gnn_G_selected, gnn_pos_selected, edge_color='grey', ax=ax)

# ノード番号（ラベル）を追加
label_offset = 0.03  # ラベルをずらす距離

# 各ノードに対して、ラベルを表示する
for i, label in enumerate(selected_nodes):
    x_pos, y_pos = gnn_pos_selected[label]
    # ラベルをずらして重ならないようにする
    ax.text(x_pos + np.random.uniform(-label_offset, label_offset), 
            y_pos + np.random.uniform(-label_offset, label_offset), 
            str(label), fontsize=25, ha='center', va='center')

# 軸ラベルを設定
ax.set_xlabel('X')
ax.set_ylabel('Y')


# # 凡例を表示（クラスタの番号と色を示す）
# from matplotlib.lines import Line2D
# legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(i / float(max(gnn_labels_selected) + 1)), markersize=10, label=f'Cluster {i}') for i in range(max(gnn_labels_selected) + 1)]
# ax.legend(handles=legend_elements, loc='upper right', title="Clusters")

# グラフを保存して表示
plt.savefig(f'{folder_path}{run_id}_{version}-kmeans-cluster-node-features-gnn', format='eps', dpi=300)
plt.savefig(f"{dir_file}/only_6.png")
